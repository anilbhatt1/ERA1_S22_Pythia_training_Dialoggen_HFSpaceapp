{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlXbVV-l874k",
        "outputId": "e006d9d7-6b39-4eb1-97aa-7365124e673b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 24 10:31:29 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Q1CD6Xgz9EfK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1ed2bfc-8a44-4b9f-ac66-17c13c14c4c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Lightning-AI/lit-gpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwL4jASC2f8_",
        "outputId": "2d86ba06-4fd1-4c69-bd2d-8b7737d1bc02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lit-gpt'...\n",
            "remote: Enumerating objects: 5144, done.\u001b[K\n",
            "remote: Counting objects: 100% (1256/1256), done.\u001b[K\n",
            "remote: Compressing objects: 100% (311/311), done.\u001b[K\n",
            "remote: Total 5144 (delta 1115), reused 965 (delta 945), pack-reused 3888\u001b[K\n",
            "Receiving objects: 100% (5144/5144), 1.76 MiB | 6.16 MiB/s, done.\n",
            "Resolving deltas: 100% (3524/3524), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/lit-gpt/')\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yhm8_d3V2nqL",
        "outputId": "739b0963-44a5-45b1-a941-e3b269f26cdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chat  finetune\tLICENSE  notebooks  quantize   requirements-all.txt  scripts   tests\t  xla\n",
            "eval  generate\tlit_gpt  pretrain   README.md  requirements.txt      setup.py  tutorials\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1woscCCm2qzZ",
        "outputId": "bb52261f-9b0e-49a7-8611-4439ad1b08ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.7/189.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.9/776.9 kB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for lightning (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Pythia 160m-deduped weights, pythia 160m our own weights & trying out both by running CLI style scripts to ensure initial working"
      ],
      "metadata": {
        "id": "vmGlYfR542h9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r '/content/gdrive/MyDrive/ERA1/s22_Sagemaker/out/redpajama/version_1' '/content'\n",
        "#!unzip -q '/content/gdrive/MyDrive/ERA1/s22_Sagemaker/S22.zip' -d '/content'\n",
        "#!unzip -q '/content/gdrive/MyDrive/ERA1/s22_Sagemaker/tokenizer_config_download_essentials.zip' -d '/content'"
      ],
      "metadata": {
        "id": "Stvx8Gi54BPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# print('cuda : ', torch.cuda.is_available())\n",
        "# import glob\n",
        "# import math\n",
        "# import sys\n",
        "# import time\n",
        "# from pathlib import Path\n",
        "# from typing import Optional, Tuple, Union\n",
        "\n",
        "# import lightning as L\n",
        "# from lightning.fabric.loggers import CSVLogger\n",
        "# from lightning.fabric.strategies import FSDPStrategy\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# from tsai_gpt.model import GPT, Block, Config\n",
        "# from tsai_gpt.packed_dataset import CombinedDataset, PackedDataset\n",
        "# from tsai_gpt.speed_monitor import SpeedMonitorBase, estimate_flops, measure_flops\n",
        "# from tsai_gpt.speed_monitor import SpeedMonitorFabric as SpeedMonitor\n",
        "# from tsai_gpt.utils import chunked_cross_entropy, get_default_supported_precision, num_parameters, load_checkpoint"
      ],
      "metadata": {
        "id": "JDtlzeVx-GrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = \"pythia-160m\"\n",
        "# name = \"redpajama\"\n",
        "\n",
        "# # Hyperparameters\n",
        "# learning_rate = 6e-3\n",
        "# batch_size = 24\n",
        "# micro_batch_size = 4\n",
        "# weight_decay = 1e-1\n",
        "# beta1 = 0.9\n",
        "# beta2 = 0.95\n",
        "# grad_clip = 1.0\n",
        "# decay_lr = True\n",
        "# warmup_iters = 2000\n",
        "# min_lr = 6e-6\n",
        "\n",
        "# hparams = {k: v for k, v in locals().items() if isinstance(v, (int, float, str)) and not k.startswith(\"_\")}\n",
        "\n",
        "# precision = get_default_supported_precision(training=True)\n",
        "# fabric = L.Fabric(devices=1, strategy=\"auto\", precision=precision, loggers=None)\n",
        "# fabric.print(hparams)"
      ],
      "metadata": {
        "id": "_wL6-Zs-Bm_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Method to prepare cpu model from GPU model\n",
        "\n",
        "\n",
        "config = Config.from_name(model_name)\n",
        "model = GPT(config)\n",
        "\n",
        "gpu_model = torch.load('/content/version_1/iter-110000-ckpt.pth')\n",
        "print(gpu_model.keys())\n",
        "model.load_state_dict(gpu_model['model'])\n",
        "# Switch the model to CPU\n",
        "cpu_model = model.to('cpu')\n",
        "torch.save(cpu_model, '/content/version_1/iter-110000-ckpt_cpu.pth')\n",
        "\n",
        "!cp '/content/version_1/iter-110000-ckpt_cpu.pth' '/content/gdrive/MyDrive/ERA1/s22_Sagemaker/out/redpajama/version_1'\n",
        "'''"
      ],
      "metadata": {
        "id": "nDMt2aMyEflp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!mv '/content/version_1/iter-110000-ckpt.pth' '/content/checkpoints/meta-llama/Llama-2-7b-chat-hf/'"
      ],
      "metadata": {
        "id": "K2hJ5Wu73UMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!mv '/content/checkpoints/meta-llama/Llama-2-7b-chat-hf/iter-110000-ckpt.pth' '/content/checkpoints/meta-llama/Llama-2-7b-chat-hf/lit_model.pth'"
      ],
      "metadata": {
        "id": "TFMs_ddE56MN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ckpt_dir = '/content/checkpoints/meta-llama/Llama-2-7b-chat-hf'"
      ],
      "metadata": {
        "id": "GJ0JkCTJ6Op8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python3 /content/lit-gpt/generate/base.py --checkpoint_dir '/content/checkpoints/meta-llama/Llama-2-7b-chat-hf' --prompt \"Hello, my name is\""
      ],
      "metadata": {
        "id": "79SXpi-Z6IVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "KGxYIrH56LaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 scripts/download.py --repo_id EleutherAI/pythia-160m-deduped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vF8WJUsawDx2",
        "outputId": "f7134c9f-c5dc-4310-e241-ccd2e22b1253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching 3 files:   0% 0/3 [00:00<?, ?it/s]\n",
            "Downloading (…)okenizer_config.json: 100% 396/396 [00:00<00:00, 1.43MB/s]\n",
            "\n",
            "Downloading (…)e8e7e/tokenizer.json:   0% 0.00/2.11M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)e8e7e/tokenizer.json: 100% 2.11M/2.11M [00:00<00:00, 2.84MB/s]\n",
            "\n",
            "Downloading pytorch_model.bin:   0% 0.00/375M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading pytorch_model.bin:   3% 10.5M/375M [00:01<00:58, 6.22MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:   6% 21.0M/375M [00:02<00:34, 10.2MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:   8% 31.5M/375M [00:02<00:24, 14.3MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  11% 41.9M/375M [00:03<00:21, 15.8MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  14% 52.4M/375M [00:03<00:19, 16.7MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  17% 62.9M/375M [00:04<00:16, 19.0MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  20% 73.4M/375M [00:04<00:15, 19.0MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  22% 83.9M/375M [00:05<00:15, 18.9MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  25% 94.4M/375M [00:05<00:14, 18.8MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  28% 105M/375M [00:06<00:13, 20.2MB/s] \u001b[A\n",
            "Downloading pytorch_model.bin:  31% 115M/375M [00:06<00:13, 19.9MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  34% 126M/375M [00:07<00:12, 19.6MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  36% 136M/375M [00:07<00:11, 21.3MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  39% 147M/375M [00:08<00:11, 20.5MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  42% 157M/375M [00:08<00:10, 20.0MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  45% 168M/375M [00:09<00:09, 21.6MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  48% 178M/375M [00:09<00:09, 20.6MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  50% 189M/375M [00:10<00:09, 20.0MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  53% 199M/375M [00:10<00:08, 21.4MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  56% 210M/375M [00:11<00:07, 20.8MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  59% 220M/375M [00:11<00:07, 20.1MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  62% 231M/375M [00:12<00:06, 21.5MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  64% 241M/375M [00:12<00:06, 20.8MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  67% 252M/375M [00:13<00:05, 22.1MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  70% 262M/375M [00:13<00:05, 20.9MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  73% 273M/375M [00:14<00:05, 20.4MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  75% 283M/375M [00:14<00:04, 21.4MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  78% 294M/375M [00:15<00:03, 21.0MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  81% 304M/375M [00:15<00:03, 20.3MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  84% 315M/375M [00:16<00:02, 21.7MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  87% 325M/375M [00:16<00:02, 20.9MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  89% 336M/375M [00:17<00:01, 22.2MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  92% 346M/375M [00:17<00:01, 21.1MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  95% 357M/375M [00:18<00:00, 20.5MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin:  98% 367M/375M [00:18<00:00, 21.9MB/s]\u001b[A\n",
            "Downloading pytorch_model.bin: 100% 375M/375M [00:19<00:00, 19.6MB/s]\n",
            "Fetching 3 files: 100% 3/3 [00:21<00:00,  7.01s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/version_1/iter-110000-ckpt.pth' '/content/lit-gpt/checkpoints/EleutherAI/pythia-160m-deduped'"
      ],
      "metadata": {
        "id": "lc1PhlGVNQ_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/EleutherAI/pythia-160m-deduped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuZvO2a9wOy9",
        "outputId": "37842991-9f49-4ece-ee5f-cf67249fe46a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model config {'name': 'pythia-160m-deduped', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000}\n",
            "Processing checkpoints/EleutherAI/pythia-160m-deduped/pytorch_model.bin\n",
            "Loading 'gpt_neox.embed_in.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.0.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.0.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.0.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.0.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.0.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.1.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.1.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.1.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.1.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.1.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.1.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.1.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.1.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.1.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.1.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.1.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.1.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.2.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.2.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.2.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.2.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.2.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.2.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.2.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.2.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.2.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.2.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.2.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.2.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.3.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.3.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.3.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.3.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.3.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.3.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.3.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.3.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.3.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.3.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.3.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.3.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.4.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.4.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.4.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.4.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.4.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.4.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.4.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.4.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.4.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.4.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.4.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.4.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.5.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.5.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.5.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.5.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.5.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.5.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.5.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.5.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.5.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.5.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.5.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.5.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.6.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.6.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.6.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.6.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.6.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.6.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.6.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.6.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.6.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.6.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.6.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.6.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.7.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.7.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.7.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.7.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.7.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.7.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.7.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.7.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.7.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.7.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.7.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.7.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.8.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.8.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.8.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.8.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.8.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.8.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.8.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.8.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.8.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.8.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.8.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.8.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.9.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.9.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.9.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.9.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.9.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.9.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.9.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.9.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.9.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.9.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.9.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.9.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.10.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.10.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.10.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.10.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.10.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.10.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.10.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.10.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.10.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.10.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.10.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.10.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.11.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.11.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.11.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.11.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.11.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.11.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.11.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.11.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.11.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.11.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.11.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.11.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.final_layer_norm.weight' into RAM\n",
            "Loading 'gpt_neox.final_layer_norm.bias' into RAM\n",
            "Loading 'embed_out.weight' into RAM\n",
            "Saving converted checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tokenizers"
      ],
      "metadata": {
        "id": "kvPxiBDKxIZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With my model\n",
        "!python3 generate/base.py --prompt \"Hello, my name is\" --checkpoint_dir checkpoints/EleutherAI/pythia-160m-deduped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqB4uqGxxNPY",
        "outputId": "af4a4556-b4a7-46c1-f287-27f2cdb73386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model 'checkpoints/EleutherAI/pythia-160m-deduped/lit_model.pth' with {'name': 'pythia-160m-deduped', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
            "Time to instantiate model: 0.57 seconds.\n",
            "Time to load the model weights: 0.58 seconds.\n",
            "Seed set to 1234\n",
            "Hello, my name is exxt extremely---------------- discussed isor creatures histories SIndexoteryas penaltyet1991, strept legitim Peterson m thular Screen isor----\"}:Fed v, Armstrong assum Armstrongzieor enoughublicor youngestpro de working lawfuliss Armstrongziefrial\n",
            "Time for inference 1: 1.61 sec total, 31.06 tokens/sec\n",
            "Memory used: 0.34 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With HF model\n",
        "!python3 generate/base.py --prompt \"Hello, my name is\" --checkpoint_dir checkpoints/EleutherAI/pythia-160m-deduped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNCiJfRtxQvT",
        "outputId": "696acc8a-e9b3-46b9-a170-a41f2eb4237b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model 'checkpoints/EleutherAI/pythia-160m-deduped/lit_model.pth' with {'name': 'pythia-160m-deduped', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
            "Time to instantiate model: 0.31 seconds.\n",
            "Time to load the model weights: 0.24 seconds.\n",
            "Seed set to 1234\n",
            "Hello, my name is Mabel.\n",
            "\n",
            "I’m a new guy in my home town of Gouda. I’ve been here for two years now, and I’ve been here for more.\n",
            "\n",
            "In 2011, I transitioned to my home\n",
            "Time for inference 1: 0.88 sec total, 56.70 tokens/sec\n",
            "Memory used: 0.34 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With HF model\n",
        "!python3 generate/base.py --prompt \"India is a country of \" --checkpoint_dir checkpoints/EleutherAI/pythia-160m-deduped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrSVIzigQVqh",
        "outputId": "7128a673-aef8-4741-aada-d3f6117999ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model 'checkpoints/EleutherAI/pythia-160m-deduped/lit_model.pth' with {'name': 'pythia-160m-deduped', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
            "Time to instantiate model: 0.26 seconds.\n",
            "Time to load the model weights: 0.22 seconds.\n",
            "Seed set to 1234\n",
            "India is a country of \n",
            "a 'noble nation', 'the country of the spirit of \n",
            "society' and 'the country of the spirit of \n",
            "work'.\n",
            "\n",
            "I do not mean to imply that there is a society (I refer to the \n",
            "Time for inference 1: 0.63 sec total, 79.24 tokens/sec\n",
            "Memory used: 0.34 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With HF model\n",
        "!python3 generate/base.py --prompt \"Football is \" --checkpoint_dir checkpoints/EleutherAI/pythia-160m-deduped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCa1YlvSQY__",
        "outputId": "f2577c2e-dbae-4622-80d5-a0a7d72604b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model 'checkpoints/EleutherAI/pythia-160m-deduped/lit_model.pth' with {'name': 'pythia-160m-deduped', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
            "Time to instantiate model: 0.24 seconds.\n",
            "Time to load the model weights: 0.21 seconds.\n",
            "Seed set to 1234\n",
            "Football is \n",
            "a 'community of faith, one of the most powerful relationships in Jewish life.'\n",
            "\n",
            "The UK\n",
            "as a single country, the UK has become a nation of faith by\n",
            "its own admission; since the end of World War II, it\n",
            "Time for inference 1: 0.65 sec total, 76.53 tokens/sec\n",
            "Memory used: 0.34 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With HF model\n",
        "!python3 generate/base.py --prompt \"AI is going to \" --checkpoint_dir checkpoints/EleutherAI/pythia-160m-deduped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SReLrN3lQfEw",
        "outputId": "d0e74e8f-1677-494e-eda5-99aa1156decd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model 'checkpoints/EleutherAI/pythia-160m-deduped/lit_model.pth' with {'name': 'pythia-160m-deduped', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
            "Time to instantiate model: 0.25 seconds.\n",
            "Time to load the model weights: 0.20 seconds.\n",
            "Seed set to 1234\n",
            "AI is going to \n",
            "auckland, I'm going to see the guy in the \n",
            "the video, He's going to be back here in \n",
            "auckland from now on.\n",
            "So, I'd like to get back into the loop, and I \n",
            "Time for inference 1: 0.64 sec total, 77.98 tokens/sec\n",
            "Memory used: 0.34 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get pythia-70m-deduped weights & copy to GDRIVE"
      ],
      "metadata": {
        "id": "aZR-G8FC4cD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 scripts/download.py --repo_id EleutherAI/pythia-70m-deduped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccfb662a-fa63-48d4-8fec-3f89b278ad29",
        "id": "cnprAcja5KAq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching 3 files:   0% 0/3 [00:00<?, ?it/s]\n",
            "tokenizer.json: 100% 2.11M/2.11M [00:00<00:00, 29.6MB/s]\n",
            "\n",
            "tokenizer_config.json: 100% 396/396 [00:00<00:00, 2.93MB/s]\n",
            "\n",
            "pytorch_model.bin:   0% 0.00/166M [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model.bin:   6% 10.5M/166M [00:02<00:33, 4.70MB/s]\u001b[A\n",
            "pytorch_model.bin:  13% 21.0M/166M [00:02<00:18, 7.77MB/s]\u001b[A\n",
            "pytorch_model.bin:  19% 31.5M/166M [00:03<00:13, 9.85MB/s]\u001b[A\n",
            "pytorch_model.bin:  25% 41.9M/166M [00:04<00:09, 12.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  32% 52.4M/166M [00:04<00:08, 13.1MB/s]\u001b[A\n",
            "pytorch_model.bin:  38% 62.9M/166M [00:05<00:07, 13.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  44% 73.4M/166M [00:06<00:06, 13.6MB/s]\u001b[A\n",
            "pytorch_model.bin:  51% 83.9M/166M [00:07<00:05, 13.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  57% 94.4M/166M [00:07<00:05, 14.0MB/s]\u001b[A\n",
            "pytorch_model.bin:  63% 105M/166M [00:08<00:03, 15.5MB/s] \u001b[A\n",
            "pytorch_model.bin:  69% 115M/166M [00:09<00:03, 15.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  76% 126M/166M [00:09<00:02, 16.5MB/s]\u001b[A\n",
            "pytorch_model.bin:  82% 136M/166M [00:10<00:01, 15.7MB/s]\u001b[A\n",
            "pytorch_model.bin:  88% 147M/166M [00:11<00:01, 15.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  95% 157M/166M [00:11<00:00, 16.5MB/s]\u001b[A\n",
            "pytorch_model.bin: 100% 166M/166M [00:12<00:00, 13.7MB/s]\n",
            "Fetching 3 files: 100% 3/3 [00:13<00:00,  4.66s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/EleutherAI/pythia-70m-deduped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP_y9hY75TXm",
        "outputId": "096bd601-0769-4baf-af7b-8864b6d11efc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model config {'name': 'pythia-70m-deduped', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-70m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 6, 'n_head': 8, 'n_embd': 512, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 8, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 2048, 'rope_condense_ratio': 1, 'rope_base': 10000}\n",
            "Processing checkpoints/EleutherAI/pythia-70m-deduped/pytorch_model.bin\n",
            "Loading 'gpt_neox.embed_in.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.0.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.0.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.0.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.0.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.0.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.1.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.1.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.1.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.1.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.1.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.1.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.1.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.1.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.1.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.1.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.1.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.1.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.2.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.2.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.2.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.2.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.2.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.2.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.2.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.2.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.2.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.2.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.2.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.2.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.3.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.3.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.3.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.3.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.3.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.3.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.3.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.3.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.3.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.3.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.3.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.3.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.4.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.4.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.4.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.4.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.4.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.4.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.4.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.4.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.4.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.4.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.4.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.4.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.5.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.5.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.5.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.5.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.5.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.5.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.5.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.5.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.5.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.5.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.5.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.5.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.final_layer_norm.weight' into RAM\n",
            "Loading 'gpt_neox.final_layer_norm.bias' into RAM\n",
            "Loading 'embed_out.weight' into RAM\n",
            "Saving converted checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With HF model\n",
        "!python3 generate/base.py --prompt \"Football is \" --checkpoint_dir checkpoints/EleutherAI/pythia-70m-deduped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGkPndCF5X9y",
        "outputId": "8a2d43a5-e19a-431c-acdd-93f36a07cbec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model 'checkpoints/EleutherAI/pythia-70m-deduped/lit_model.pth' with {'name': 'pythia-70m-deduped', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-70m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 6, 'n_head': 8, 'n_embd': 512, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 8, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 2048, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
            "Time to instantiate model: 0.22 seconds.\n",
            "Time to load the model weights: 0.13 seconds.\n",
            "Seed set to 1234\n",
            "Football is \n",
            "the 'official' name, 'the country' and the only way of gaining knowledge of the world\n",
            "as it's the world's most important country. It is also a symbol of the\n",
            "practicable society. 'P.A.\n",
            "Time for inference 1: 1.23 sec total, 40.71 tokens/sec\n",
            "Memory used: 0.15 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r '/content/lit-gpt/checkpoints/EleutherAI/pythia-70m-deduped' '/content/gdrive/MyDrive/ERA1/s22_Sagemaker/dialog-gen-hf-spaces-app/checkpoints/EleutherAI'"
      ],
      "metadata": {
        "id": "lCFYXsBG6RJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get pythia-410m-deduped weights & copy to GDRIVE\n"
      ],
      "metadata": {
        "id": "_XgxqwSY68Zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 scripts/download.py --repo_id EleutherAI/pythia-410m-deduped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5053378b-2cf6-4373-b981-4487219400b5",
        "id": "NKbiyBST6-MO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching 3 files:   0% 0/3 [00:00<?, ?it/s]\n",
            "tokenizer_config.json: 100% 396/396 [00:00<00:00, 2.38MB/s]\n",
            "\n",
            "tokenizer.json: 100% 2.11M/2.11M [00:00<00:00, 129MB/s]\n",
            "\n",
            "pytorch_model.bin:   0% 0.00/911M [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model.bin:   1% 10.5M/911M [00:02<02:55, 5.13MB/s]\u001b[A\n",
            "pytorch_model.bin:   2% 21.0M/911M [00:02<01:45, 8.42MB/s]\u001b[A\n",
            "pytorch_model.bin:   3% 31.5M/911M [00:03<01:14, 11.8MB/s]\u001b[A\n",
            "pytorch_model.bin:   5% 41.9M/911M [00:03<01:06, 13.0MB/s]\u001b[A\n",
            "pytorch_model.bin:   6% 52.4M/911M [00:04<01:02, 13.8MB/s]\u001b[A\n",
            "pytorch_model.bin:   7% 62.9M/911M [00:05<00:53, 16.0MB/s]\u001b[A\n",
            "pytorch_model.bin:   8% 73.4M/911M [00:05<00:53, 15.7MB/s]\u001b[A\n",
            "pytorch_model.bin:   9% 83.9M/911M [00:06<00:56, 14.7MB/s]\u001b[A\n",
            "pytorch_model.bin:  10% 94.4M/911M [00:07<00:52, 15.7MB/s]\u001b[A\n",
            "pytorch_model.bin:  12% 105M/911M [00:07<00:51, 15.6MB/s] \u001b[A\n",
            "pytorch_model.bin:  13% 115M/911M [00:08<00:46, 17.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  14% 126M/911M [00:08<00:47, 16.6MB/s]\u001b[A\n",
            "pytorch_model.bin:  15% 136M/911M [00:09<00:47, 16.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  16% 147M/911M [00:10<00:43, 17.7MB/s]\u001b[A\n",
            "pytorch_model.bin:  17% 157M/911M [00:10<00:44, 16.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  18% 168M/911M [00:11<00:45, 16.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  20% 178M/911M [00:11<00:41, 17.8MB/s]\u001b[A\n",
            "pytorch_model.bin:  21% 189M/911M [00:12<00:42, 17.0MB/s]\u001b[A\n",
            "pytorch_model.bin:  22% 199M/911M [00:13<00:43, 16.5MB/s]\u001b[A\n",
            "pytorch_model.bin:  23% 210M/911M [00:13<00:39, 17.8MB/s]\u001b[A\n",
            "pytorch_model.bin:  24% 220M/911M [00:14<00:40, 17.1MB/s]\u001b[A\n",
            "pytorch_model.bin:  25% 231M/911M [00:14<00:37, 18.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  26% 241M/911M [00:15<00:38, 17.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  28% 252M/911M [00:16<00:39, 16.8MB/s]\u001b[A\n",
            "pytorch_model.bin:  29% 262M/911M [00:16<00:36, 18.0MB/s]\u001b[A\n",
            "pytorch_model.bin:  30% 273M/911M [00:17<00:37, 17.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  31% 283M/911M [00:17<00:34, 18.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  32% 294M/911M [00:18<00:35, 17.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  33% 304M/911M [00:19<00:36, 16.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  35% 315M/911M [00:19<00:32, 18.1MB/s]\u001b[A\n",
            "pytorch_model.bin:  36% 325M/911M [00:20<00:33, 17.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  37% 336M/911M [00:20<00:31, 18.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  38% 346M/911M [00:21<00:32, 17.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  39% 357M/911M [00:22<00:32, 16.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  40% 367M/911M [00:22<00:30, 18.0MB/s]\u001b[A\n",
            "pytorch_model.bin:  41% 377M/911M [00:23<00:30, 17.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  43% 388M/911M [00:23<00:28, 18.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  44% 398M/911M [00:24<00:29, 17.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  45% 409M/911M [00:25<00:29, 16.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  46% 419M/911M [00:25<00:27, 18.1MB/s]\u001b[A\n",
            "pytorch_model.bin:  47% 430M/911M [00:26<00:27, 17.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  48% 440M/911M [00:26<00:25, 18.5MB/s]\u001b[A\n",
            "pytorch_model.bin:  49% 451M/911M [00:27<00:26, 17.5MB/s]\u001b[A\n",
            "pytorch_model.bin:  51% 461M/911M [00:28<00:26, 16.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  52% 472M/911M [00:28<00:24, 18.1MB/s]\u001b[A\n",
            "pytorch_model.bin:  53% 482M/911M [00:29<00:24, 17.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  54% 493M/911M [00:29<00:22, 18.6MB/s]\u001b[A\n",
            "pytorch_model.bin:  55% 503M/911M [00:30<00:23, 17.6MB/s]\u001b[A\n",
            "pytorch_model.bin:  56% 514M/911M [00:31<00:23, 16.8MB/s]\u001b[A\n",
            "pytorch_model.bin:  58% 524M/911M [00:31<00:21, 18.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  59% 535M/911M [00:32<00:21, 17.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  60% 545M/911M [00:32<00:21, 16.7MB/s]\u001b[A\n",
            "pytorch_model.bin:  61% 556M/911M [00:33<00:19, 18.0MB/s]\u001b[A\n",
            "pytorch_model.bin:  62% 566M/911M [00:34<00:20, 17.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  63% 577M/911M [00:34<00:18, 18.4MB/s]\u001b[A\n",
            "pytorch_model.bin:  64% 587M/911M [00:35<00:18, 17.5MB/s]\u001b[A\n",
            "pytorch_model.bin:  66% 598M/911M [00:35<00:18, 16.8MB/s]\u001b[A\n",
            "pytorch_model.bin:  67% 608M/911M [00:36<00:16, 18.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  68% 619M/911M [00:37<00:17, 17.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  69% 629M/911M [00:37<00:17, 16.5MB/s]\u001b[A\n",
            "pytorch_model.bin:  70% 640M/911M [00:38<00:16, 16.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  71% 650M/911M [00:38<00:14, 17.7MB/s]\u001b[A\n",
            "pytorch_model.bin:  72% 661M/911M [00:39<00:14, 17.0MB/s]\u001b[A\n",
            "pytorch_model.bin:  74% 671M/911M [00:40<00:16, 14.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  75% 682M/911M [00:41<00:15, 15.0MB/s]\u001b[A\n",
            "pytorch_model.bin:  76% 692M/911M [00:41<00:14, 15.1MB/s]\u001b[A\n",
            "pytorch_model.bin:  77% 703M/911M [00:42<00:13, 15.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  78% 713M/911M [00:43<00:13, 15.3MB/s]\u001b[A\n",
            "pytorch_model.bin:  79% 724M/911M [00:44<00:13, 14.1MB/s]\u001b[A\n",
            "pytorch_model.bin:  81% 734M/911M [00:44<00:11, 15.7MB/s]\u001b[A\n",
            "pytorch_model.bin:  82% 744M/911M [00:45<00:10, 15.7MB/s]\u001b[A\n",
            "pytorch_model.bin:  83% 755M/911M [00:45<00:10, 15.6MB/s]\u001b[A\n",
            "pytorch_model.bin:  84% 765M/911M [00:47<00:11, 12.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  85% 776M/911M [00:47<00:10, 12.5MB/s]\u001b[A\n",
            "pytorch_model.bin:  86% 786M/911M [00:48<00:10, 12.5MB/s]\u001b[A\n",
            "pytorch_model.bin:  87% 797M/911M [00:49<00:08, 13.0MB/s]\u001b[A\n",
            "pytorch_model.bin:  89% 807M/911M [00:50<00:08, 12.8MB/s]\u001b[A\n",
            "pytorch_model.bin:  90% 818M/911M [00:51<00:07, 13.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  91% 828M/911M [00:51<00:06, 13.6MB/s]\u001b[A\n",
            "pytorch_model.bin:  92% 839M/911M [00:52<00:05, 13.2MB/s]\u001b[A\n",
            "pytorch_model.bin:  93% 849M/911M [00:53<00:04, 13.7MB/s]\u001b[A\n",
            "pytorch_model.bin:  94% 860M/911M [00:54<00:03, 14.0MB/s]\u001b[A\n",
            "pytorch_model.bin:  95% 870M/911M [00:54<00:03, 13.5MB/s]\u001b[A\n",
            "pytorch_model.bin:  97% 881M/911M [00:55<00:02, 13.9MB/s]\u001b[A\n",
            "pytorch_model.bin:  98% 891M/911M [00:56<00:01, 14.1MB/s]\u001b[A\n",
            "pytorch_model.bin:  99% 902M/911M [00:57<00:00, 13.4MB/s]\u001b[A\n",
            "pytorch_model.bin: 100% 911M/911M [00:57<00:00, 15.7MB/s]\n",
            "Fetching 3 files: 100% 3/3 [00:59<00:00, 19.96s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 scripts/convert_hf_checkpoint.py --checkpoint_dir checkpoints/EleutherAI/pythia-410m-deduped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e6add15-6eda-47cb-e757-4ac5c6e6eac6",
        "id": "eFiXPg3P6-MO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model config {'name': 'pythia-410m-deduped', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-410m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 24, 'n_head': 16, 'n_embd': 1024, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 16, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 4096, 'rope_condense_ratio': 1, 'rope_base': 10000}\n",
            "Processing checkpoints/EleutherAI/pythia-410m-deduped/pytorch_model.bin\n",
            "Loading 'gpt_neox.embed_in.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.0.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.0.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.0.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.0.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.0.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.0.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.1.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.1.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.1.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.1.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.1.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.1.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.1.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.1.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.1.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.1.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.1.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.1.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.2.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.2.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.2.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.2.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.2.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.2.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.2.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.2.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.2.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.2.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.2.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.2.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.3.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.3.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.3.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.3.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.3.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.3.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.3.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.3.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.3.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.3.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.3.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.3.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.4.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.4.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.4.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.4.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.4.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.4.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.4.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.4.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.4.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.4.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.4.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.4.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.5.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.5.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.5.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.5.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.5.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.5.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.5.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.5.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.5.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.5.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.5.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.5.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.6.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.6.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.6.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.6.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.6.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.6.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.6.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.6.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.6.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.6.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.6.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.6.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.7.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.7.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.7.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.7.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.7.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.7.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.7.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.7.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.7.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.7.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.7.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.7.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.8.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.8.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.8.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.8.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.8.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.8.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.8.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.8.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.8.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.8.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.8.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.8.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.9.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.9.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.9.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.9.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.9.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.9.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.9.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.9.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.9.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.9.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.9.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.9.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.10.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.10.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.10.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.10.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.10.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.10.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.10.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.10.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.10.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.10.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.10.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.10.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.11.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.11.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.11.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.11.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.11.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.11.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.11.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.11.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.11.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.11.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.11.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.11.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.12.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.12.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.12.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.12.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.12.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.12.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.12.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.12.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.12.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.12.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.12.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.12.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.13.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.13.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.13.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.13.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.13.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.13.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.13.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.13.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.13.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.13.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.13.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.13.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.14.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.14.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.14.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.14.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.14.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.14.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.14.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.14.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.14.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.14.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.14.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.14.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.15.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.15.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.15.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.15.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.15.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.15.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.15.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.15.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.15.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.15.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.15.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.15.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.16.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.16.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.16.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.16.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.16.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.16.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.16.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.16.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.16.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.16.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.16.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.16.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.17.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.17.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.17.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.17.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.17.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.17.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.17.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.17.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.17.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.17.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.17.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.17.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.18.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.18.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.18.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.18.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.18.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.18.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.18.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.18.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.18.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.18.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.18.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.18.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.19.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.19.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.19.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.19.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.19.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.19.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.19.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.19.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.19.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.19.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.19.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.19.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.20.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.20.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.20.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.20.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.20.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.20.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.20.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.20.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.20.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.20.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.20.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.20.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.21.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.21.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.21.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.21.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.21.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.21.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.21.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.21.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.21.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.21.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.21.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.21.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.22.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.22.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.22.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.22.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.22.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.22.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.22.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.22.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.22.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.22.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.22.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.22.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.23.input_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.23.input_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.23.post_attention_layernorm.weight' into RAM\n",
            "Loading 'gpt_neox.layers.23.post_attention_layernorm.bias' into RAM\n",
            "Loading 'gpt_neox.layers.23.attention.query_key_value.weight' into RAM\n",
            "Loading 'gpt_neox.layers.23.attention.query_key_value.bias' into RAM\n",
            "Loading 'gpt_neox.layers.23.attention.dense.weight' into RAM\n",
            "Loading 'gpt_neox.layers.23.attention.dense.bias' into RAM\n",
            "Loading 'gpt_neox.layers.23.mlp.dense_h_to_4h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.23.mlp.dense_h_to_4h.bias' into RAM\n",
            "Loading 'gpt_neox.layers.23.mlp.dense_4h_to_h.weight' into RAM\n",
            "Loading 'gpt_neox.layers.23.mlp.dense_4h_to_h.bias' into RAM\n",
            "Loading 'gpt_neox.final_layer_norm.weight' into RAM\n",
            "Loading 'gpt_neox.final_layer_norm.bias' into RAM\n",
            "Loading 'embed_out.weight' into RAM\n",
            "Saving converted checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With HF model\n",
        "!python3 generate/base.py --prompt \"Football is \" --checkpoint_dir checkpoints/EleutherAI/pythia-410m-deduped"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20cc670c-f583-469a-88f6-bd2ec04ba7a2",
        "id": "yeielHJU6-MP"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model 'checkpoints/EleutherAI/pythia-410m-deduped/lit_model.pth' with {'name': 'pythia-410m-deduped', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-410m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 24, 'n_head': 16, 'n_embd': 1024, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 16, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 4096, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
            "Time to instantiate model: 0.06 seconds.\n",
            "Time to load the model weights: 0.42 seconds.\n",
            "Seed set to 1234\n",
            "Football is \n",
            "a game, I say, 'Gentlemen, I only have 5 minutes, I need all\n",
            "the time in the world for this, and I will give you this\n",
            "word. I do not know how much further the game is going\n",
            "Time for inference 1: 1.31 sec total, 38.03 tokens/sec\n",
            "Memory used: 0.83 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r '/content/lit-gpt/checkpoints/EleutherAI/pythia-410m-deduped' '/content/gdrive/MyDrive/ERA1/s22_Sagemaker/dialog-gen-hf-spaces-app/checkpoints/EleutherAI'"
      ],
      "metadata": {
        "id": "VqVt55qf6-MP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}