{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlXbVV-l874k",
        "outputId": "019bbbb5-021a-417f-d331-0eea6cffe019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov  9 09:06:16 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Q1CD6Xgz9EfK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c354bf75-488b-4c90-87cb-b6efc18dae20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r '/content/gdrive/MyDrive/ERA1/s22_Sagemaker' '.'\n",
        "!unzip -q '/content/gdrive/MyDrive/ERA1/s22_Sagemaker/data.zip' -d '/content'\n",
        "!unzip -q '/content/s22_Sagemaker/S22.zip' -d '/content'\n",
        "!unzip -q '/content/s22_Sagemaker/tokenizer_config_download_essentials.zip' -d '/content'"
      ],
      "metadata": {
        "id": "hV5awo-_VRfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DulZVKQVAG2",
        "outputId": "1531db1f-d0ea-46db-8a81-2d802c90c9b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "import glob\n",
        "import math\n",
        "import sys\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Union\n",
        "\n",
        "import lightning as L\n",
        "from lightning.fabric.loggers import CSVLogger\n",
        "from lightning.fabric.strategies import FSDPStrategy\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from tsai_gpt.model import GPT, Block, Config\n",
        "from tsai_gpt.packed_dataset import CombinedDataset, PackedDataset\n",
        "from tsai_gpt.speed_monitor import SpeedMonitorBase, estimate_flops, measure_flops\n",
        "from tsai_gpt.speed_monitor import SpeedMonitorFabric as SpeedMonitor\n",
        "from tsai_gpt.utils import chunked_cross_entropy, get_default_supported_precision, num_parameters, load_checkpoint"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPWguZ8lU4rs",
        "outputId": "83640470-85ff-439e-bbf1-256065e813c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r '/content/s22_Sagemaker/out/redpajama/version_1/iter-099655-ckpt.pth' '/content/out/redpajama'"
      ],
      "metadata": {
        "id": "zm5p7cXsG14p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"pythia-160m\"\n",
        "name = \"redpajama\"\n",
        "# out_dir = Path(\"/out\") / name\n",
        "out_dir = Path('/content/out/redpajama/version_1')\n",
        "save_interval = 1000\n",
        "eval_interval = 1000\n",
        "eval_iters = 100\n",
        "log_interval = 100\n",
        "print(out_dir)\n",
        "max(out_dir.glob(\"*.pth\"), key=lambda p: int(p.name.split(\"-\")[1]))"
      ],
      "metadata": {
        "id": "vSpHthuCXXkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04f661bf-2457-4788-aa93-6b752537143c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/out/redpajama/version_1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/out/redpajama/version_1/iter-099655-ckpt.pth')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 6e-3\n",
        "batch_size = 24\n",
        "micro_batch_size = 4\n",
        "gradient_accumulation_steps = batch_size // micro_batch_size\n",
        "assert gradient_accumulation_steps > 0\n",
        "#max_iters = 600000  # num_epochs * (epoch_size // micro_batch_size) // devices\n",
        "max_iters = 1_10_000\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0\n",
        "decay_lr = True\n",
        "warmup_iters = 2000\n",
        "lr_decay_iters = max_iters\n",
        "min_lr = 6e-6"
      ],
      "metadata": {
        "id": "zNCBJ03iXbWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data proportions from https://arxiv.org/pdf/2302.13971.pdf Table 1\n",
        "data_config = [\n",
        "    (\"arxiv\", 2.5),\n",
        "    (\"book\", 4.5),\n",
        "    (\"c4\", 15.0),\n",
        "    (\"cc\", 67.0),\n",
        "    (\"github\", 4.5),\n",
        "    (\"stackexchange\", 2.0),\n",
        "    (\"wikipedia\", 4.5),\n",
        "]"
      ],
      "metadata": {
        "id": "3bsemdBOXdkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hparams = {k: v for k, v in locals().items() if isinstance(v, (int, float, str)) and not k.startswith(\"_\")}\n",
        "logger = CSVLogger(\"out\", name, flush_logs_every_n_steps=log_interval)\n",
        "\n",
        "\n",
        "def setup(\n",
        "    devices: int = 4,\n",
        "    train_data_dir: Path = Path(\"data/redpajama_sample\"),\n",
        "    val_data_dir: Optional[Path] = None,\n",
        "    precision: Optional[str] = None,\n",
        "    resume: Union[bool, Path] = True,\n",
        ") -> None:\n",
        "    precision = precision or get_default_supported_precision(training=True)\n",
        "\n",
        "    if devices > 1:\n",
        "        strategy = FSDPStrategy(\n",
        "            auto_wrap_policy={Block},\n",
        "            activation_checkpointing_policy={Block},\n",
        "            state_dict_type=\"full\",\n",
        "            limit_all_gathers=True,\n",
        "            cpu_offload=False,\n",
        "        )\n",
        "    else:\n",
        "        strategy = \"auto\"\n",
        "\n",
        "    print(strategy)\n",
        "    fabric = L.Fabric(devices=devices, strategy=strategy, precision=precision, loggers=logger)\n",
        "    fabric.print(hparams)\n",
        "    fabric.launch(main, train_data_dir, val_data_dir, resume)"
      ],
      "metadata": {
        "id": "JPbdx8wUXf96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_copy = None"
      ],
      "metadata": {
        "id": "m_yrQhPCXkya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(fabric: L.Fabric, train_data_dir: Path, val_data_dir: Path, resume: Union[bool, Path]) -> None:\n",
        "    global model_copy\n",
        "    speed_monitor = SpeedMonitor(fabric, window_size=50, time_unit=\"seconds\")\n",
        "\n",
        "    if fabric.global_rank == 0:\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    config = Config.from_name(model_name)\n",
        "\n",
        "    train_dataloader, val_dataloader = create_dataloaders(\n",
        "        batch_size=micro_batch_size,\n",
        "        block_size=config.block_size,\n",
        "        fabric=fabric,\n",
        "        train_data_dir=train_data_dir,\n",
        "        val_data_dir=val_data_dir,\n",
        "        seed=(1337 + fabric.global_rank),\n",
        "    )\n",
        "    if val_dataloader is None:\n",
        "        train_dataloader = fabric.setup_dataloaders(train_dataloader)\n",
        "    else:\n",
        "        train_dataloader, val_dataloader = fabric.setup_dataloaders(train_dataloader, val_dataloader)\n",
        "\n",
        "    fabric.seed_everything(1337)  # same seed for every process to init model (FSDP)\n",
        "\n",
        "    fabric.print(f\"Loading model with {config.__dict__}\")\n",
        "    t0 = time.perf_counter()\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    def _init_weights(module: nn.Module) -> None:\n",
        "            \"\"\"Meant to be used with `gpt.apply(gpt._init_weights)`.\"\"\"\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    torch.nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    with fabric.init_module(empty_init=True):\n",
        "        model = GPT(config)\n",
        "        model.apply(_init_weights)\n",
        "    model.apply(_init_weights)\n",
        "\n",
        "\n",
        "    # checkpoint_path = Path(\"out/redpajama/iter-000999-ckpt.pth\")\n",
        "\n",
        "    # load_checkpoint(fabric, model, checkpoint_path)\n",
        "\n",
        "    # print(model.transformer.h[0].mlp.fc.weight)\n",
        "\n",
        "    fabric.print(f\"Time to instantiate model: {time.perf_counter() - t0:.02f} seconds.\")\n",
        "    fabric.print(f\"Total parameters {num_parameters(model):,}\")\n",
        "\n",
        "    model = fabric.setup(model)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(beta1, beta2), foreach=False\n",
        "    )\n",
        "\n",
        "    # model_copy = model\n",
        "\n",
        "    optimizer = fabric.setup_optimizers(optimizer)\n",
        "\n",
        "    state = {\"model\": model, \"optimizer\": optimizer, \"hparams\": hparams, \"iter_num\": 0, \"step_count\": 0}\n",
        "\n",
        "    if resume is True:\n",
        "        resume = max(out_dir.glob(\"*.pth\"), key=lambda p: int(p.name.split(\"-\")[1]))\n",
        "        fabric.print(f\"Resume is true\")\n",
        "    if resume:\n",
        "        fabric.print(f\"Resuming training from {resume}\")\n",
        "        fabric.load(resume, state)\n",
        "\n",
        "    train_time = time.perf_counter()\n",
        "    train(fabric, state, train_dataloader, val_dataloader, speed_monitor)\n",
        "    fabric.print(f\"Training time: {(time.perf_counter()-train_time):.2f}s\")\n",
        "    if fabric.device.type == \"cuda\":\n",
        "        fabric.print(f\"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\")\n"
      ],
      "metadata": {
        "id": "ana0-imAXrCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    fabric: L.Fabric,\n",
        "    state: dict,\n",
        "    train_dataloader: DataLoader,\n",
        "    val_dataloader: DataLoader,\n",
        "    speed_monitor: SpeedMonitorBase,\n",
        ") -> None:\n",
        "    model = state[\"model\"]\n",
        "    optimizer = state[\"optimizer\"]\n",
        "\n",
        "    if val_dataloader is not None:\n",
        "        validate(fabric, model, val_dataloader)  # sanity check\n",
        "\n",
        "    with torch.device(\"meta\"):\n",
        "        meta_model = GPT(model.config)\n",
        "        # \"estimated\" is not as precise as \"measured\". Estimated is optimistic but widely used in the wild.\n",
        "        # When comparing MFU or FLOP numbers with other projects that use estimated FLOPs,\n",
        "        # consider passing `SpeedMonitor(flops_per_batch=estimated_flops)` instead\n",
        "        estimated_flops = estimate_flops(meta_model) * micro_batch_size\n",
        "        fabric.print(f\"Estimated TFLOPs: {estimated_flops * fabric.world_size / 1e12:.2f}\")\n",
        "        x = torch.randint(0, 1, (micro_batch_size, model.max_seq_length))\n",
        "        measured_flops = measure_flops(meta_model, x)\n",
        "        fabric.print(f\"Measured TFLOPs: {measured_flops * fabric.world_size / 1e12:.2f}\")\n",
        "        del meta_model, x\n",
        "\n",
        "    total_lengths = 0\n",
        "    total_t0 = time.perf_counter()\n",
        "\n",
        "    for state[\"iter_num\"], train_data in enumerate(train_dataloader, state[\"iter_num\"]):\n",
        "        if state[\"iter_num\"] >= max_iters:\n",
        "            checkpoint_path = out_dir / f\"iter-{state['iter_num']:06d}-ckpt.pth\"\n",
        "            fabric.print(f\"iter {state['iter_num']} step {state['step_count']}: loss {loss.item():.4f}\")\n",
        "            fabric.print(f\"Saving checkpoint to {str(checkpoint_path)!r}\")\n",
        "            fabric.save(checkpoint_path, state)\n",
        "            break\n",
        "\n",
        "        # determine and set the learning rate for this iteration\n",
        "        lr = get_lr(state[\"iter_num\"]) if decay_lr else learning_rate\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr\n",
        "\n",
        "        iter_t0 = time.perf_counter()\n",
        "\n",
        "        input_ids = train_data[:, 0 : model.max_seq_length].contiguous()\n",
        "        targets = train_data[:, 1 : model.max_seq_length + 1].contiguous()\n",
        "\n",
        "        is_accumulating = (state[\"iter_num\"] + 1) % gradient_accumulation_steps != 0\n",
        "        with fabric.no_backward_sync(model, enabled=is_accumulating):\n",
        "            logits = model(input_ids)\n",
        "            loss = chunked_cross_entropy(logits, targets, chunk_size=0)\n",
        "            fabric.backward(loss / gradient_accumulation_steps)\n",
        "\n",
        "        # return\n",
        "\n",
        "        if not is_accumulating:\n",
        "            # print(f'state[\"iter_num\"] : {state[\"iter_num\"]}, gradient_accumulation_steps : {gradient_accumulation_steps}')\n",
        "            # fabric.clip_gradients(model, optimizer, max_norm=grad_clip)#, norm_type=\"inf\")\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            state[\"step_count\"] += 1\n",
        "\n",
        "        t1 = time.perf_counter()\n",
        "        total_lengths += input_ids.size(1)\n",
        "        speed_monitor.on_train_batch_end(\n",
        "            (state[\"iter_num\"] + 1) * micro_batch_size,\n",
        "            t1 - total_t0,\n",
        "            # this assumes that device FLOPs are the same and that all devices have the same batch size\n",
        "            fabric.world_size,\n",
        "            flops_per_batch=measured_flops,\n",
        "            lengths=total_lengths,\n",
        "        )\n",
        "        if state[\"iter_num\"] % log_interval == 0:\n",
        "            fabric.print(\n",
        "                f\"iter {state['iter_num']} step {state['step_count']}: loss {loss.item():.4f}, LR: {lr:.6f}, iter time:\"\n",
        "                f\" {(t1 - iter_t0) * 1000:.2f}ms{' (optimizer.step)' if not is_accumulating else ''}\"\n",
        "            )\n",
        "\n",
        "        if val_dataloader is not None and not is_accumulating and state[\"step_count\"] % eval_interval == 0:\n",
        "            t0 = time.perf_counter()\n",
        "            val_loss = validate(fabric, model, val_dataloader)\n",
        "            t1 = time.perf_counter() - t0\n",
        "            speed_monitor.eval_end(t1)\n",
        "            fabric.print(f\"step {state['iter_num']}: val loss {val_loss.item():.4f}, val time: {t1 * 1000:.2f}ms\")\n",
        "            fabric.barrier()\n",
        "        if not is_accumulating and state[\"step_count\"] % save_interval == 0:\n",
        "            checkpoint_path = out_dir / f\"iter-{state['iter_num']:06d}-ckpt.pth\"\n",
        "            fabric.print(f\"{state['iter_num']} - Saving checkpoint to {str(checkpoint_path)!r}\")\n",
        "            fabric.save(checkpoint_path, state)\n",
        "            fabric.print(\n",
        "                f\"iter {state['iter_num']} step {state['step_count']}: loss {loss.item():.4f}, LR: {lr:.6f}, iter time:\"\n",
        "            )"
      ],
      "metadata": {
        "id": "_VAGnu7JXwgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def validate(fabric: L.Fabric, model: torch.nn.Module, val_dataloader: DataLoader) -> torch.Tensor:\n",
        "    fabric.print(\"Validating ...\")\n",
        "    model.eval()\n",
        "\n",
        "    losses = torch.zeros(eval_iters, device=fabric.device)\n",
        "    for k, val_data in enumerate(val_dataloader):\n",
        "        input_ids = val_data[:, 0 : model.max_seq_length].contiguous()\n",
        "        targets = val_data[:, 1 : model.max_seq_length + 1].contiguous()\n",
        "        logits = model(input_ids)\n",
        "        losses[k] = chunked_cross_entropy(logits, targets, chunk_size=0)\n",
        "    out = losses.mean()\n",
        "\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "oncUI0R3X0cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(\n",
        "    batch_size: int, block_size: int, data_dir: Path, fabric: L.Fabric, shuffle: bool = True, seed: int = 12345\n",
        ") -> DataLoader:\n",
        "    datasets = []\n",
        "    for prefix, _ in data_config:\n",
        "        filenames = glob.glob(str(data_dir / f\"{prefix}*\"))\n",
        "        dataset = PackedDataset(\n",
        "            filenames,\n",
        "            n_chunks=4,\n",
        "            block_size=block_size,\n",
        "            shuffle=shuffle,\n",
        "            seed=seed,\n",
        "            num_processes=fabric.world_size,\n",
        "            process_rank=fabric.global_rank,\n",
        "        )\n",
        "        datasets.append(dataset)\n",
        "\n",
        "    if not datasets:\n",
        "        raise RuntimeError(\n",
        "            f\"No data found at {data_dir}. Make sure you ran prepare_redpajama.py to create the dataset.\"\n",
        "        )\n",
        "\n",
        "    weights = [weight for _, weight in data_config]\n",
        "    sum_weights = sum(weights)\n",
        "    weights = [el / sum_weights for el in weights]\n",
        "\n",
        "    combined_dataset = CombinedDataset(datasets=datasets, seed=seed, weights=weights)\n",
        "\n",
        "    return DataLoader(combined_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "sH6vADuxX3Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloaders(\n",
        "    batch_size: int,\n",
        "    block_size: int,\n",
        "    fabric: L.Fabric,\n",
        "    train_data_dir: Path = Path(\"data/redpajama_sample\"),\n",
        "    val_data_dir: Optional[Path] = None,\n",
        "    seed: int = 12345,\n",
        ") -> Tuple[DataLoader, DataLoader]:\n",
        "    # Increase by one because we need the next word as well\n",
        "    effective_block_size = block_size + 1\n",
        "    train_dataloader = create_dataloader(\n",
        "        batch_size=batch_size,\n",
        "        block_size=effective_block_size,\n",
        "        fabric=fabric,\n",
        "        data_dir=train_data_dir,\n",
        "        shuffle=True,\n",
        "        seed=seed,\n",
        "    )\n",
        "    val_dataloader = (\n",
        "        create_dataloader(\n",
        "            batch_size=batch_size,\n",
        "            block_size=effective_block_size,\n",
        "            fabric=fabric,\n",
        "            data_dir=val_data_dir,\n",
        "            shuffle=False,\n",
        "            seed=seed,\n",
        "        )\n",
        "        if val_data_dir\n",
        "        else None\n",
        "    )\n",
        "    return train_dataloader, val_dataloader"
      ],
      "metadata": {
        "id": "6WSEr8qpX6Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lr(it: int) -> float:\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)"
      ],
      "metadata": {
        "id": "iKRoyXnfX9Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "setup(\n",
        "    devices=1,\n",
        "    train_data_dir=Path(\"data/lit-redpajama-sample\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi222_tAEyJw",
        "outputId": "a4882cd0-0808-489d-cf59-adadbe8d7613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Using 16-bit Automatic Mixed Precision (AMP)\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using 16-bit Automatic Mixed Precision (AMP)\n",
            "INFO: Seed set to 1337\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 1337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "auto\n",
            "{'model_name': 'pythia-160m', 'name': 'redpajama', 'save_interval': 1000, 'eval_interval': 1000, 'eval_iters': 100, 'log_interval': 100, 'learning_rate': 0.006, 'batch_size': 24, 'micro_batch_size': 4, 'gradient_accumulation_steps': 6, 'max_iters': 110000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 2000, 'lr_decay_iters': 110000, 'min_lr': 6e-06}\n",
            "Loading model with {'name': 'pythia-160m', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
            "Time to instantiate model: 5.29 seconds.\n",
            "Total parameters 162,322,944\n",
            "Resume is true\n",
            "Resuming training from /content/out/redpajama/version_1/iter-099655-ckpt.pth\n",
            "Estimated TFLOPs: 11.07\n",
            "Measured TFLOPs: 7.93\n",
            "iter 99700 step 32007: loss 3.0335, LR: 0.000140, iter time: 518.14ms\n",
            "iter 99800 step 32024: loss 3.0012, LR: 0.000137, iter time: 528.95ms\n",
            "iter 99900 step 32041: loss 3.6841, LR: 0.000134, iter time: 561.42ms\n",
            "iter 100000 step 32057: loss 3.4150, LR: 0.000132, iter time: 558.20ms\n",
            "iter 100100 step 32074: loss 3.4157, LR: 0.000129, iter time: 560.99ms\n",
            "iter 100200 step 32091: loss 3.5829, LR: 0.000127, iter time: 575.85ms\n",
            "iter 100300 step 32107: loss 3.4228, LR: 0.000125, iter time: 566.19ms\n",
            "iter 100400 step 32124: loss 2.5879, LR: 0.000122, iter time: 571.45ms\n",
            "iter 100500 step 32141: loss 3.4645, LR: 0.000120, iter time: 589.71ms\n",
            "iter 100600 step 32157: loss 3.2932, LR: 0.000117, iter time: 569.81ms\n",
            "iter 100700 step 32174: loss 3.1776, LR: 0.000115, iter time: 574.64ms\n",
            "iter 100800 step 32191: loss 3.3568, LR: 0.000113, iter time: 588.99ms\n",
            "iter 100900 step 32207: loss 3.7185, LR: 0.000110, iter time: 576.44ms\n",
            "iter 101000 step 32224: loss 3.5023, LR: 0.000108, iter time: 569.89ms\n",
            "iter 101100 step 32241: loss 3.1913, LR: 0.000106, iter time: 590.65ms\n",
            "iter 101200 step 32257: loss 3.6538, LR: 0.000104, iter time: 566.37ms\n",
            "iter 101300 step 32274: loss 3.7185, LR: 0.000101, iter time: 571.78ms\n",
            "iter 101400 step 32291: loss 3.4297, LR: 0.000099, iter time: 587.38ms\n",
            "iter 101500 step 32307: loss 3.6871, LR: 0.000097, iter time: 575.47ms\n",
            "iter 101600 step 32324: loss 3.0886, LR: 0.000095, iter time: 572.69ms\n",
            "iter 101700 step 32341: loss 3.0210, LR: 0.000093, iter time: 594.11ms\n",
            "iter 101800 step 32357: loss 3.6552, LR: 0.000091, iter time: 576.60ms\n",
            "iter 101900 step 32374: loss 3.4339, LR: 0.000089, iter time: 571.84ms\n",
            "iter 102000 step 32391: loss 3.8232, LR: 0.000087, iter time: 593.25ms\n",
            "iter 102100 step 32407: loss 3.7177, LR: 0.000085, iter time: 577.18ms\n",
            "iter 102200 step 32424: loss 3.1109, LR: 0.000083, iter time: 573.42ms\n",
            "iter 102300 step 32441: loss 3.5923, LR: 0.000081, iter time: 587.71ms\n",
            "iter 102400 step 32457: loss 3.2041, LR: 0.000079, iter time: 572.89ms\n",
            "iter 102500 step 32474: loss 3.5795, LR: 0.000077, iter time: 575.17ms\n",
            "iter 102600 step 32491: loss 3.7194, LR: 0.000075, iter time: 593.68ms\n",
            "iter 102700 step 32507: loss 3.3335, LR: 0.000073, iter time: 574.62ms\n",
            "iter 102800 step 32524: loss 3.8226, LR: 0.000071, iter time: 572.50ms\n",
            "iter 102900 step 32541: loss 3.7346, LR: 0.000070, iter time: 592.93ms\n",
            "iter 103000 step 32557: loss 3.7021, LR: 0.000068, iter time: 584.63ms\n",
            "iter 103100 step 32574: loss 3.5397, LR: 0.000066, iter time: 572.50ms\n",
            "iter 103200 step 32591: loss 3.2951, LR: 0.000064, iter time: 597.33ms\n",
            "iter 103300 step 32607: loss 3.2534, LR: 0.000063, iter time: 578.38ms\n",
            "iter 103400 step 32624: loss 3.3085, LR: 0.000061, iter time: 576.94ms\n",
            "iter 103500 step 32641: loss 3.5297, LR: 0.000059, iter time: 590.24ms\n",
            "iter 103600 step 32657: loss 3.6148, LR: 0.000058, iter time: 580.63ms\n",
            "iter 103700 step 32674: loss 3.5536, LR: 0.000056, iter time: 573.90ms\n",
            "iter 103800 step 32691: loss 3.8456, LR: 0.000055, iter time: 600.77ms\n",
            "iter 103900 step 32707: loss 2.5347, LR: 0.000053, iter time: 561.12ms\n",
            "iter 104000 step 32724: loss 3.3118, LR: 0.000052, iter time: 583.59ms\n",
            "iter 104100 step 32741: loss 3.5492, LR: 0.000050, iter time: 599.26ms\n",
            "iter 104200 step 32757: loss 3.7380, LR: 0.000049, iter time: 583.90ms\n",
            "iter 104300 step 32774: loss 3.3387, LR: 0.000047, iter time: 582.91ms\n",
            "iter 104400 step 32791: loss 3.8401, LR: 0.000046, iter time: 600.77ms\n",
            "iter 104500 step 32807: loss 3.7317, LR: 0.000044, iter time: 588.41ms\n",
            "iter 104600 step 32824: loss 3.3024, LR: 0.000043, iter time: 581.53ms\n",
            "iter 104700 step 32841: loss 3.6157, LR: 0.000042, iter time: 602.82ms\n",
            "iter 104800 step 32857: loss 3.1901, LR: 0.000040, iter time: 587.03ms\n",
            "iter 104900 step 32874: loss 2.8585, LR: 0.000039, iter time: 578.73ms\n",
            "iter 105000 step 32891: loss 3.2715, LR: 0.000038, iter time: 594.56ms\n",
            "iter 105100 step 32907: loss 3.2090, LR: 0.000036, iter time: 584.88ms\n",
            "iter 105200 step 32924: loss 3.2544, LR: 0.000035, iter time: 581.71ms\n",
            "iter 105300 step 32941: loss 3.6695, LR: 0.000034, iter time: 597.96ms\n",
            "iter 105400 step 32957: loss 3.7449, LR: 0.000033, iter time: 586.52ms\n",
            "iter 105500 step 32974: loss 3.7504, LR: 0.000032, iter time: 584.33ms\n",
            "iter 105600 step 32991: loss 3.8371, LR: 0.000031, iter time: 598.61ms\n",
            "105653 - Saving checkpoint to '/content/out/redpajama/version_1/iter-105653-ckpt.pth'\n",
            "iter 105653 step 33000: loss 3.5474, LR: 0.000030, iter time:\n",
            "iter 105700 step 33007: loss 3.7942, LR: 0.000029, iter time: 609.72ms\n",
            "iter 105800 step 33024: loss 3.5169, LR: 0.000028, iter time: 581.05ms\n",
            "iter 105900 step 33041: loss 3.1722, LR: 0.000027, iter time: 595.91ms\n",
            "iter 106000 step 33057: loss 3.4072, LR: 0.000026, iter time: 584.50ms\n",
            "iter 106100 step 33074: loss 3.6794, LR: 0.000025, iter time: 582.13ms\n",
            "iter 106200 step 33091: loss 3.9028, LR: 0.000024, iter time: 599.68ms\n",
            "iter 106300 step 33107: loss 3.7219, LR: 0.000023, iter time: 584.49ms\n",
            "iter 106400 step 33124: loss 3.1429, LR: 0.000022, iter time: 578.47ms\n",
            "iter 106500 step 33141: loss 3.8743, LR: 0.000022, iter time: 594.84ms\n",
            "iter 106600 step 33157: loss 3.8719, LR: 0.000021, iter time: 580.54ms\n",
            "iter 106700 step 33174: loss 2.8981, LR: 0.000020, iter time: 564.16ms\n",
            "iter 106800 step 33191: loss 3.4617, LR: 0.000019, iter time: 600.49ms\n",
            "iter 106900 step 33207: loss 3.1895, LR: 0.000018, iter time: 588.10ms\n",
            "iter 107000 step 33224: loss 4.0308, LR: 0.000017, iter time: 587.76ms\n",
            "iter 107100 step 33241: loss 3.4762, LR: 0.000017, iter time: 594.73ms\n",
            "iter 107200 step 33257: loss 3.3745, LR: 0.000016, iter time: 588.49ms\n",
            "iter 107300 step 33274: loss 4.0452, LR: 0.000015, iter time: 583.01ms\n",
            "iter 107400 step 33291: loss 4.0566, LR: 0.000015, iter time: 597.98ms\n",
            "iter 107500 step 33307: loss 3.9562, LR: 0.000014, iter time: 582.77ms\n",
            "iter 107600 step 33324: loss 3.9728, LR: 0.000013, iter time: 588.00ms\n",
            "iter 107700 step 33341: loss 3.7071, LR: 0.000013, iter time: 591.78ms\n",
            "iter 107800 step 33357: loss 3.6433, LR: 0.000012, iter time: 587.24ms\n",
            "iter 107900 step 33374: loss 3.5922, LR: 0.000012, iter time: 584.54ms\n",
            "iter 108000 step 33391: loss 3.3519, LR: 0.000011, iter time: 597.35ms\n",
            "iter 108100 step 33407: loss 3.7380, LR: 0.000011, iter time: 581.16ms\n",
            "iter 108200 step 33424: loss 4.0163, LR: 0.000010, iter time: 584.28ms\n",
            "iter 108300 step 33441: loss 3.9703, LR: 0.000010, iter time: 598.36ms\n",
            "iter 108400 step 33457: loss 3.8097, LR: 0.000009, iter time: 584.44ms\n",
            "iter 108500 step 33474: loss 3.1010, LR: 0.000009, iter time: 582.92ms\n",
            "iter 108600 step 33491: loss 4.0367, LR: 0.000008, iter time: 595.79ms\n",
            "iter 108700 step 33507: loss 3.6085, LR: 0.000008, iter time: 581.37ms\n",
            "iter 108800 step 33524: loss 3.8296, LR: 0.000008, iter time: 582.44ms\n",
            "iter 108900 step 33541: loss 3.6619, LR: 0.000008, iter time: 593.39ms\n",
            "iter 109000 step 33557: loss 3.7753, LR: 0.000007, iter time: 588.46ms\n",
            "iter 109100 step 33574: loss 3.6457, LR: 0.000007, iter time: 580.81ms\n",
            "iter 109200 step 33591: loss 3.4556, LR: 0.000007, iter time: 592.52ms\n",
            "iter 109300 step 33607: loss 3.4625, LR: 0.000007, iter time: 585.32ms\n",
            "iter 109400 step 33624: loss 2.8268, LR: 0.000006, iter time: 556.32ms\n",
            "iter 109500 step 33641: loss 2.9204, LR: 0.000006, iter time: 596.74ms\n",
            "iter 109600 step 33657: loss 3.6323, LR: 0.000006, iter time: 583.16ms\n",
            "iter 109700 step 33674: loss 3.7149, LR: 0.000006, iter time: 583.71ms\n",
            "iter 109800 step 33691: loss 3.8845, LR: 0.000006, iter time: 598.17ms\n",
            "iter 109900 step 33707: loss 3.2313, LR: 0.000006, iter time: 586.69ms\n",
            "iter 110000 step 33724: loss 3.1331\n",
            "Saving checkpoint to '/content/out/redpajama/version_1/iter-110000-ckpt.pth'\n",
            "Training time: 6120.23s\n",
            "Memory used: 12.25 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "7xmGI6U6Ukt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "setup(\n",
        "    devices=1,\n",
        "    train_data_dir=Path(\"data/lit-redpajama-sample\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmeAQzku7w2N",
        "outputId": "216798da-f717-4017-e4bc-a9ae70e4f5e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Using 16-bit Automatic Mixed Precision (AMP)\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using 16-bit Automatic Mixed Precision (AMP)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "auto\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Seed set to 1337\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 1337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model_name': 'pythia-160m', 'name': 'redpajama', 'save_interval': 1000, 'eval_interval': 1000, 'eval_iters': 100, 'log_interval': 100, 'learning_rate': 0.006, 'batch_size': 4, 'micro_batch_size': 1, 'gradient_accumulation_steps': 4, 'max_iters': 95000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 2000, 'lr_decay_iters': 95000, 'min_lr': 6e-06}\n",
            "Loading model with {'name': 'pythia-160m', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
            "Time to instantiate model: 0.02 seconds.\n",
            "Total parameters 162,322,944\n",
            "Resume is true\n",
            "Resuming training from /content/out/redpajama/version_1/iter-085000-ckpt.pth\n",
            "Estimated TFLOPs: 2.77\n",
            "Measured TFLOPs: 1.98\n",
            "iter 85000 step 28336: loss 3.2153, LR: 0.000175, iter time: 65.64ms\n",
            "iter 85100 step 28361: loss 4.0457, LR: 0.000172, iter time: 45.26ms\n",
            "iter 85200 step 28386: loss 2.8277, LR: 0.000169, iter time: 48.07ms\n",
            "iter 85300 step 28411: loss 3.7649, LR: 0.000165, iter time: 48.13ms\n",
            "iter 85400 step 28436: loss 3.7543, LR: 0.000162, iter time: 49.12ms\n",
            "iter 85500 step 28461: loss 4.1385, LR: 0.000159, iter time: 48.86ms\n",
            "iter 85600 step 28486: loss 3.7506, LR: 0.000156, iter time: 48.71ms\n",
            "iter 85700 step 28511: loss 4.3447, LR: 0.000153, iter time: 49.86ms\n",
            "iter 85800 step 28536: loss 3.5541, LR: 0.000150, iter time: 47.93ms\n",
            "iter 85900 step 28561: loss 3.9696, LR: 0.000146, iter time: 48.40ms\n",
            "iter 86000 step 28586: loss 3.3674, LR: 0.000143, iter time: 57.49ms\n",
            "iter 86100 step 28611: loss 4.3278, LR: 0.000140, iter time: 49.37ms\n",
            "iter 86200 step 28636: loss 3.7268, LR: 0.000137, iter time: 48.65ms\n",
            "iter 86300 step 28661: loss 3.6252, LR: 0.000134, iter time: 46.03ms\n",
            "iter 86400 step 28686: loss 3.8370, LR: 0.000132, iter time: 47.86ms\n",
            "iter 86500 step 28711: loss 3.5820, LR: 0.000129, iter time: 49.52ms\n",
            "iter 86600 step 28736: loss 3.9853, LR: 0.000126, iter time: 49.10ms\n",
            "iter 86700 step 28761: loss 3.7221, LR: 0.000123, iter time: 50.04ms\n",
            "iter 86800 step 28786: loss 3.0910, LR: 0.000120, iter time: 49.84ms\n",
            "iter 86900 step 28811: loss 3.8078, LR: 0.000117, iter time: 48.85ms\n",
            "iter 87000 step 28836: loss 3.7582, LR: 0.000115, iter time: 47.98ms\n",
            "iter 87100 step 28861: loss 3.8693, LR: 0.000112, iter time: 50.20ms\n",
            "iter 87200 step 28886: loss 2.0044, LR: 0.000109, iter time: 46.91ms\n",
            "iter 87300 step 28911: loss 3.9312, LR: 0.000107, iter time: 56.68ms\n",
            "iter 87400 step 28936: loss 3.5688, LR: 0.000104, iter time: 49.36ms\n",
            "iter 87500 step 28961: loss 2.6667, LR: 0.000102, iter time: 49.70ms\n",
            "iter 87600 step 28986: loss 3.3233, LR: 0.000099, iter time: 49.33ms\n",
            "87655 - Saving checkpoint to '/content/out/redpajama/version_1/iter-087655-ckpt.pth'\n",
            "iter 87655 step 29000: loss 3.2685, LR: 0.000098, iter time:\n",
            "iter 87700 step 29011: loss 3.6302, LR: 0.000097, iter time: 49.60ms\n",
            "iter 87800 step 29036: loss 4.8401, LR: 0.000094, iter time: 49.53ms\n",
            "iter 87900 step 29061: loss 3.7116, LR: 0.000092, iter time: 49.60ms\n",
            "iter 88000 step 29086: loss 3.4910, LR: 0.000089, iter time: 49.59ms\n",
            "iter 88100 step 29111: loss 4.2344, LR: 0.000087, iter time: 47.83ms\n",
            "iter 88200 step 29136: loss 3.7664, LR: 0.000085, iter time: 49.51ms\n",
            "iter 88300 step 29161: loss 2.4625, LR: 0.000082, iter time: 47.87ms\n",
            "iter 88400 step 29186: loss 3.4073, LR: 0.000080, iter time: 62.45ms\n",
            "iter 88500 step 29211: loss 3.7806, LR: 0.000078, iter time: 51.31ms\n",
            "iter 88600 step 29236: loss 3.3648, LR: 0.000076, iter time: 55.47ms\n",
            "iter 88700 step 29261: loss 3.3106, LR: 0.000074, iter time: 58.07ms\n",
            "iter 88800 step 29286: loss 3.9702, LR: 0.000071, iter time: 46.03ms\n",
            "iter 88900 step 29311: loss 1.3367, LR: 0.000069, iter time: 49.79ms\n",
            "iter 89000 step 29336: loss 3.5218, LR: 0.000067, iter time: 49.50ms\n",
            "iter 89100 step 29361: loss 3.4821, LR: 0.000065, iter time: 50.10ms\n",
            "iter 89200 step 29386: loss 4.7859, LR: 0.000063, iter time: 46.48ms\n",
            "iter 89300 step 29411: loss 3.6089, LR: 0.000061, iter time: 49.48ms\n",
            "iter 89400 step 29436: loss 3.6698, LR: 0.000059, iter time: 49.18ms\n",
            "iter 89500 step 29461: loss 3.6569, LR: 0.000058, iter time: 49.61ms\n",
            "iter 89600 step 29486: loss 2.9901, LR: 0.000056, iter time: 52.52ms\n",
            "iter 89700 step 29511: loss 2.3665, LR: 0.000054, iter time: 47.92ms\n",
            "iter 89800 step 29536: loss 4.1011, LR: 0.000052, iter time: 57.16ms\n",
            "iter 89900 step 29561: loss 2.2449, LR: 0.000050, iter time: 49.35ms\n",
            "iter 90000 step 29586: loss 3.8913, LR: 0.000049, iter time: 50.40ms\n",
            "iter 90100 step 29611: loss 3.2747, LR: 0.000047, iter time: 49.65ms\n",
            "iter 90200 step 29636: loss 3.9463, LR: 0.000045, iter time: 49.71ms\n",
            "iter 90300 step 29661: loss 3.4108, LR: 0.000044, iter time: 49.03ms\n",
            "iter 90400 step 29686: loss 3.8194, LR: 0.000042, iter time: 49.19ms\n",
            "iter 90500 step 29711: loss 3.8139, LR: 0.000041, iter time: 50.24ms\n",
            "iter 90600 step 29736: loss 3.2994, LR: 0.000039, iter time: 50.02ms\n",
            "iter 90700 step 29761: loss 3.9734, LR: 0.000038, iter time: 47.78ms\n",
            "iter 90800 step 29786: loss 3.6549, LR: 0.000036, iter time: 47.97ms\n",
            "iter 90900 step 29811: loss 3.9749, LR: 0.000035, iter time: 49.11ms\n",
            "iter 91000 step 29836: loss 3.8608, LR: 0.000033, iter time: 55.79ms\n",
            "iter 91100 step 29861: loss 3.6884, LR: 0.000032, iter time: 48.86ms\n",
            "iter 91200 step 29886: loss 1.0481, LR: 0.000031, iter time: 49.82ms\n",
            "iter 91300 step 29911: loss 3.1250, LR: 0.000029, iter time: 49.66ms\n",
            "iter 91400 step 29936: loss 3.8946, LR: 0.000028, iter time: 51.83ms\n",
            "iter 91500 step 29961: loss 3.5657, LR: 0.000027, iter time: 49.32ms\n",
            "iter 91600 step 29986: loss 3.7358, LR: 0.000026, iter time: 47.72ms\n",
            "91655 - Saving checkpoint to '/content/out/redpajama/version_1/iter-091655-ckpt.pth'\n",
            "iter 91655 step 30000: loss 2.3088, LR: 0.000025, iter time:\n",
            "iter 91700 step 30011: loss 3.6381, LR: 0.000025, iter time: 48.84ms\n",
            "iter 91800 step 30036: loss 3.8841, LR: 0.000023, iter time: 49.27ms\n",
            "iter 91900 step 30061: loss 3.6680, LR: 0.000022, iter time: 47.10ms\n",
            "iter 92000 step 30086: loss 3.5051, LR: 0.000021, iter time: 49.32ms\n",
            "iter 92100 step 30111: loss 3.2320, LR: 0.000020, iter time: 49.64ms\n",
            "iter 92200 step 30136: loss 3.7411, LR: 0.000019, iter time: 50.37ms\n",
            "iter 92300 step 30161: loss 3.9192, LR: 0.000018, iter time: 55.71ms\n",
            "iter 92400 step 30186: loss 3.9343, LR: 0.000018, iter time: 51.03ms\n",
            "iter 92500 step 30211: loss 3.3035, LR: 0.000017, iter time: 49.70ms\n",
            "iter 92600 step 30236: loss 2.7376, LR: 0.000016, iter time: 49.26ms\n",
            "iter 92700 step 30261: loss 3.4556, LR: 0.000015, iter time: 50.72ms\n",
            "iter 92800 step 30286: loss 2.7175, LR: 0.000014, iter time: 49.91ms\n",
            "iter 92900 step 30311: loss 4.0061, LR: 0.000014, iter time: 49.58ms\n",
            "iter 93000 step 30336: loss 3.7987, LR: 0.000013, iter time: 50.00ms\n",
            "iter 93100 step 30361: loss 3.2248, LR: 0.000012, iter time: 49.71ms\n",
            "iter 93200 step 30386: loss 2.9334, LR: 0.000012, iter time: 49.28ms\n",
            "iter 93300 step 30411: loss 3.7671, LR: 0.000011, iter time: 50.43ms\n",
            "iter 93400 step 30436: loss 3.8348, LR: 0.000010, iter time: 54.74ms\n",
            "iter 93500 step 30461: loss 3.5640, LR: 0.000010, iter time: 48.27ms\n",
            "iter 93600 step 30486: loss 3.4685, LR: 0.000009, iter time: 55.58ms\n",
            "iter 93700 step 30511: loss 3.3060, LR: 0.000009, iter time: 49.86ms\n",
            "iter 93800 step 30536: loss 3.8902, LR: 0.000008, iter time: 48.64ms\n",
            "iter 93900 step 30561: loss 3.4271, LR: 0.000008, iter time: 49.27ms\n",
            "iter 94000 step 30586: loss 3.8205, LR: 0.000008, iter time: 49.07ms\n",
            "iter 94100 step 30611: loss 2.9260, LR: 0.000007, iter time: 47.07ms\n",
            "iter 94200 step 30636: loss 3.6361, LR: 0.000007, iter time: 49.79ms\n",
            "iter 94300 step 30661: loss 3.3663, LR: 0.000007, iter time: 50.46ms\n",
            "iter 94400 step 30686: loss 3.2038, LR: 0.000007, iter time: 49.17ms\n",
            "iter 94500 step 30711: loss 2.8518, LR: 0.000006, iter time: 52.15ms\n",
            "iter 94600 step 30736: loss 4.0638, LR: 0.000006, iter time: 51.25ms\n",
            "iter 94700 step 30761: loss 3.6978, LR: 0.000006, iter time: 55.90ms\n",
            "iter 94800 step 30786: loss 3.6548, LR: 0.000006, iter time: 57.78ms\n",
            "iter 94900 step 30811: loss 3.7629, LR: 0.000006, iter time: 49.11ms\n",
            "iter 95000 step 30836: loss 3.4736\n",
            "Saving checkpoint to '/content/out/redpajama/version_1/iter-095000-ckpt.pth'\n",
            "Training time: 1622.97s\n",
            "Memory used: 12.38 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "setup(\n",
        "    devices=1,\n",
        "    train_data_dir=Path(\"data/lit-redpajama-sample\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGhENXbrj7gp",
        "outputId": "4687a1f2-faef-40cd-8cb9-648ec37eb22c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Using 16-bit Automatic Mixed Precision (AMP)\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using 16-bit Automatic Mixed Precision (AMP)\n",
            "INFO: Seed set to 1337\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 1337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "auto\n",
            "{'model_name': 'pythia-160m', 'name': 'redpajama', 'save_interval': 1000, 'eval_interval': 1000, 'eval_iters': 100, 'log_interval': 100, 'learning_rate': 0.006, 'batch_size': 16, 'micro_batch_size': 4, 'gradient_accumulation_steps': 4, 'max_iters': 85000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 2000, 'lr_decay_iters': 85000, 'min_lr': 6e-06}\n",
            "Loading model with {'name': 'pythia-160m', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
            "Time to instantiate model: 0.02 seconds.\n",
            "Total parameters 162,322,944\n",
            "Resume is true\n",
            "Resuming training from /content/out/redpajama/version_1/iter-075000-ckpt.pth\n",
            "Estimated TFLOPs: 11.07\n",
            "Measured TFLOPs: 7.93\n",
            "iter 75000 step 25836: loss 3.3007, LR: 0.000218, iter time: 569.37ms\n",
            "iter 75100 step 25861: loss 3.5898, LR: 0.000214, iter time: 602.54ms\n",
            "iter 75200 step 25886: loss 3.3681, LR: 0.000210, iter time: 586.89ms\n",
            "iter 75300 step 25911: loss 4.1100, LR: 0.000206, iter time: 581.26ms\n",
            "iter 75400 step 25936: loss 3.5468, LR: 0.000202, iter time: 596.53ms\n",
            "iter 75500 step 25961: loss 3.5691, LR: 0.000198, iter time: 588.26ms\n",
            "iter 75600 step 25986: loss 3.6144, LR: 0.000194, iter time: 588.61ms\n",
            "75655 - Saving checkpoint to '/content/out/redpajama/version_1/iter-075655-ckpt.pth'\n",
            "iter 75655 step 26000: loss 3.5257, LR: 0.000192, iter time:\n",
            "iter 75700 step 26011: loss 4.0089, LR: 0.000190, iter time: 604.42ms\n",
            "iter 75800 step 26036: loss 3.8206, LR: 0.000186, iter time: 582.52ms\n",
            "iter 75900 step 26061: loss 3.5609, LR: 0.000182, iter time: 587.93ms\n",
            "iter 76000 step 26086: loss 3.4111, LR: 0.000178, iter time: 590.68ms\n",
            "iter 76100 step 26111: loss 3.6456, LR: 0.000174, iter time: 585.92ms\n",
            "iter 76200 step 26136: loss 4.2173, LR: 0.000171, iter time: 588.31ms\n",
            "iter 76300 step 26161: loss 3.9683, LR: 0.000167, iter time: 591.96ms\n",
            "iter 76400 step 26186: loss 3.7975, LR: 0.000163, iter time: 589.54ms\n",
            "iter 76500 step 26211: loss 3.5780, LR: 0.000160, iter time: 591.09ms\n",
            "iter 76600 step 26236: loss 4.0170, LR: 0.000156, iter time: 586.48ms\n",
            "iter 76700 step 26261: loss 3.7521, LR: 0.000153, iter time: 591.17ms\n",
            "iter 76800 step 26286: loss 3.8764, LR: 0.000149, iter time: 594.18ms\n",
            "iter 76900 step 26311: loss 3.4009, LR: 0.000146, iter time: 590.73ms\n",
            "iter 77000 step 26336: loss 2.7633, LR: 0.000142, iter time: 565.72ms\n",
            "iter 77100 step 26361: loss 3.1874, LR: 0.000139, iter time: 590.26ms\n",
            "iter 77200 step 26386: loss 3.8260, LR: 0.000136, iter time: 596.51ms\n",
            "iter 77300 step 26411: loss 3.4257, LR: 0.000132, iter time: 594.49ms\n",
            "iter 77400 step 26436: loss 3.8532, LR: 0.000129, iter time: 590.90ms\n",
            "iter 77500 step 26461: loss 3.7266, LR: 0.000126, iter time: 589.67ms\n",
            "iter 77600 step 26486: loss 3.7773, LR: 0.000123, iter time: 588.56ms\n",
            "iter 77700 step 26511: loss 3.0608, LR: 0.000120, iter time: 587.03ms\n",
            "iter 77800 step 26536: loss 3.9179, LR: 0.000117, iter time: 589.00ms\n",
            "iter 77900 step 26561: loss 3.8051, LR: 0.000114, iter time: 590.74ms\n",
            "iter 78000 step 26586: loss 3.5676, LR: 0.000111, iter time: 591.00ms\n",
            "iter 78100 step 26611: loss 3.1450, LR: 0.000108, iter time: 591.74ms\n",
            "iter 78200 step 26636: loss 3.7768, LR: 0.000105, iter time: 590.90ms\n",
            "iter 78300 step 26661: loss 3.6849, LR: 0.000102, iter time: 594.95ms\n",
            "iter 78400 step 26686: loss 3.1361, LR: 0.000099, iter time: 592.34ms\n",
            "iter 78500 step 26711: loss 4.0082, LR: 0.000096, iter time: 592.51ms\n",
            "iter 78600 step 26736: loss 2.9164, LR: 0.000094, iter time: 591.46ms\n",
            "iter 78700 step 26761: loss 3.4005, LR: 0.000091, iter time: 587.52ms\n",
            "iter 78800 step 26786: loss 3.0250, LR: 0.000088, iter time: 591.32ms\n",
            "iter 78900 step 26811: loss 3.7702, LR: 0.000086, iter time: 586.19ms\n",
            "iter 79000 step 26836: loss 3.8072, LR: 0.000083, iter time: 588.40ms\n",
            "iter 79100 step 26861: loss 3.5219, LR: 0.000080, iter time: 591.92ms\n",
            "iter 79200 step 26886: loss 2.8563, LR: 0.000078, iter time: 590.16ms\n",
            "iter 79300 step 26911: loss 3.0561, LR: 0.000075, iter time: 590.47ms\n",
            "iter 79400 step 26936: loss 3.3665, LR: 0.000073, iter time: 592.13ms\n",
            "iter 79500 step 26961: loss 3.5939, LR: 0.000071, iter time: 592.24ms\n",
            "iter 79600 step 26986: loss 3.6792, LR: 0.000068, iter time: 586.69ms\n",
            "79655 - Saving checkpoint to '/content/out/redpajama/version_1/iter-079655-ckpt.pth'\n",
            "iter 79655 step 27000: loss 3.9226, LR: 0.000067, iter time:\n",
            "iter 79700 step 27011: loss 3.3995, LR: 0.000066, iter time: 605.77ms\n",
            "iter 79800 step 27036: loss 3.4683, LR: 0.000064, iter time: 582.62ms\n",
            "iter 79900 step 27061: loss 3.0296, LR: 0.000062, iter time: 587.23ms\n",
            "iter 80000 step 27086: loss 3.5305, LR: 0.000060, iter time: 590.54ms\n",
            "iter 80100 step 27111: loss 3.9837, LR: 0.000057, iter time: 589.36ms\n",
            "iter 80200 step 27136: loss 3.7805, LR: 0.000055, iter time: 593.05ms\n",
            "iter 80300 step 27161: loss 3.8996, LR: 0.000053, iter time: 592.91ms\n",
            "iter 80400 step 27186: loss 3.7074, LR: 0.000051, iter time: 592.55ms\n",
            "iter 80500 step 27211: loss 3.7619, LR: 0.000049, iter time: 591.49ms\n",
            "iter 80600 step 27236: loss 3.7331, LR: 0.000047, iter time: 590.03ms\n",
            "iter 80700 step 27261: loss 3.6030, LR: 0.000046, iter time: 590.95ms\n",
            "iter 80800 step 27286: loss 3.8849, LR: 0.000044, iter time: 589.44ms\n",
            "iter 80900 step 27311: loss 3.8217, LR: 0.000042, iter time: 592.53ms\n",
            "iter 81000 step 27336: loss 3.7878, LR: 0.000040, iter time: 588.77ms\n",
            "iter 81100 step 27361: loss 3.7455, LR: 0.000039, iter time: 587.59ms\n",
            "iter 81200 step 27386: loss 3.7457, LR: 0.000037, iter time: 586.27ms\n",
            "iter 81300 step 27411: loss 3.7034, LR: 0.000035, iter time: 587.90ms\n",
            "iter 81400 step 27436: loss 4.2355, LR: 0.000034, iter time: 584.40ms\n",
            "iter 81500 step 27461: loss 3.5903, LR: 0.000032, iter time: 590.47ms\n",
            "iter 81600 step 27486: loss 3.7860, LR: 0.000031, iter time: 585.81ms\n",
            "iter 81700 step 27511: loss 3.3994, LR: 0.000029, iter time: 593.72ms\n",
            "iter 81800 step 27536: loss 3.8010, LR: 0.000028, iter time: 592.41ms\n",
            "iter 81900 step 27561: loss 3.4685, LR: 0.000027, iter time: 590.53ms\n",
            "iter 82000 step 27586: loss 3.8173, LR: 0.000025, iter time: 589.29ms\n",
            "iter 82100 step 27611: loss 3.6029, LR: 0.000024, iter time: 592.28ms\n",
            "iter 82200 step 27636: loss 3.5066, LR: 0.000023, iter time: 587.65ms\n",
            "iter 82300 step 27661: loss 3.9640, LR: 0.000022, iter time: 590.68ms\n",
            "iter 82400 step 27686: loss 4.0040, LR: 0.000021, iter time: 588.71ms\n",
            "iter 82500 step 27711: loss 3.6525, LR: 0.000019, iter time: 591.98ms\n",
            "iter 82600 step 27736: loss 2.8184, LR: 0.000018, iter time: 571.00ms\n",
            "iter 82700 step 27761: loss 3.6494, LR: 0.000017, iter time: 592.64ms\n",
            "iter 82800 step 27786: loss 3.7571, LR: 0.000016, iter time: 589.44ms\n",
            "iter 82900 step 27811: loss 3.3788, LR: 0.000015, iter time: 590.11ms\n",
            "iter 83000 step 27836: loss 3.8826, LR: 0.000015, iter time: 590.94ms\n",
            "iter 83100 step 27861: loss 3.8673, LR: 0.000014, iter time: 587.54ms\n",
            "iter 83200 step 27886: loss 3.7430, LR: 0.000013, iter time: 591.67ms\n",
            "iter 83300 step 27911: loss 3.8131, LR: 0.000012, iter time: 592.77ms\n",
            "iter 83400 step 27936: loss 3.5681, LR: 0.000011, iter time: 591.95ms\n",
            "iter 83500 step 27961: loss 3.9598, LR: 0.000011, iter time: 593.50ms\n",
            "iter 83600 step 27986: loss 3.9938, LR: 0.000010, iter time: 596.43ms\n",
            "83655 - Saving checkpoint to '/content/out/redpajama/version_1/iter-083655-ckpt.pth'\n",
            "iter 83655 step 28000: loss 3.3896, LR: 0.000010, iter time:\n",
            "iter 83700 step 28011: loss 3.6133, LR: 0.000010, iter time: 588.23ms\n",
            "iter 83800 step 28036: loss 3.6473, LR: 0.000009, iter time: 586.90ms\n",
            "iter 83900 step 28061: loss 3.4544, LR: 0.000009, iter time: 579.90ms\n",
            "iter 84000 step 28086: loss 3.7876, LR: 0.000008, iter time: 583.27ms\n",
            "iter 84100 step 28111: loss 3.7057, LR: 0.000008, iter time: 578.89ms\n",
            "iter 84200 step 28136: loss 3.5263, LR: 0.000007, iter time: 570.22ms\n",
            "iter 84300 step 28161: loss 3.4842, LR: 0.000007, iter time: 574.01ms\n",
            "iter 84400 step 28186: loss 3.7340, LR: 0.000007, iter time: 576.58ms\n",
            "iter 84500 step 28211: loss 3.4448, LR: 0.000007, iter time: 578.58ms\n",
            "iter 84600 step 28236: loss 3.6035, LR: 0.000006, iter time: 575.80ms\n",
            "iter 84700 step 28261: loss 3.4370, LR: 0.000006, iter time: 577.59ms\n",
            "iter 84800 step 28286: loss 3.6697, LR: 0.000006, iter time: 581.93ms\n",
            "iter 84900 step 28311: loss 3.2079, LR: 0.000006, iter time: 582.12ms\n",
            "iter 85000 step 28336: loss 2.6077\n",
            "Saving checkpoint to '/content/out/redpajama/version_1/iter-085000-ckpt.pth'\n",
            "Training time: 5969.37s\n",
            "Memory used: 12.38 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "setup(\n",
        "    devices=1,\n",
        "    train_data_dir=Path(\"data/lit-redpajama-sample\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr4dO7XELRQH",
        "outputId": "ed7d1ce3-9c8f-4bb1-f2d0-17db4f00d8e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Using 16-bit Automatic Mixed Precision (AMP)\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using 16-bit Automatic Mixed Precision (AMP)\n",
            "INFO: Seed set to 1337\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 1337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "auto\n",
            "{'model_name': 'pythia-160m', 'name': 'redpajama', 'save_interval': 1000, 'eval_interval': 1000, 'eval_iters': 100, 'log_interval': 100, 'learning_rate': 0.006, 'batch_size': 12, 'micro_batch_size': 4, 'gradient_accumulation_steps': 3, 'max_iters': 75000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 2000, 'lr_decay_iters': 75000, 'min_lr': 6e-06}\n",
            "Loading model with {'name': 'pythia-160m', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
            "Time to instantiate model: 5.24 seconds.\n",
            "Total parameters 162,322,944\n",
            "Resume is true\n",
            "Resuming training from /content/out/redpajama/version_1/iter-065000-ckpt.pth\n",
            "Estimated TFLOPs: 11.07\n",
            "Measured TFLOPs: 7.93\n",
            "iter 65000 step 22503: loss 3.5272, LR: 0.000279, iter time: 3155.23ms (optimizer.step)\n",
            "iter 65100 step 22536: loss 3.8800, LR: 0.000274, iter time: 549.52ms\n",
            "iter 65200 step 22569: loss 3.5211, LR: 0.000269, iter time: 585.85ms\n",
            "iter 65300 step 22603: loss 4.4381, LR: 0.000263, iter time: 612.61ms (optimizer.step)\n",
            "iter 65400 step 22636: loss 3.7894, LR: 0.000258, iter time: 587.72ms\n",
            "iter 65500 step 22669: loss 3.7555, LR: 0.000253, iter time: 580.21ms\n",
            "iter 65600 step 22703: loss 3.8042, LR: 0.000248, iter time: 613.12ms (optimizer.step)\n",
            "iter 65700 step 22736: loss 4.1944, LR: 0.000243, iter time: 593.18ms\n",
            "iter 65800 step 22769: loss 3.9522, LR: 0.000238, iter time: 579.98ms\n",
            "iter 65900 step 22803: loss 3.6935, LR: 0.000233, iter time: 613.69ms (optimizer.step)\n",
            "iter 66000 step 22836: loss 3.6914, LR: 0.000228, iter time: 591.38ms\n",
            "iter 66100 step 22869: loss 3.7746, LR: 0.000223, iter time: 588.71ms\n",
            "iter 66200 step 22903: loss 4.4003, LR: 0.000218, iter time: 615.03ms (optimizer.step)\n",
            "iter 66300 step 22936: loss 4.0912, LR: 0.000214, iter time: 592.82ms\n",
            "iter 66400 step 22969: loss 3.9329, LR: 0.000209, iter time: 580.14ms\n",
            "66491 - Saving checkpoint to '/content/out/redpajama/version_1/iter-066491-ckpt.pth'\n",
            "iter 66491 step 23000: loss 3.5831, LR: 0.000205, iter time:\n",
            "iter 66500 step 23003: loss 3.7178, LR: 0.000204, iter time: 605.05ms (optimizer.step)\n",
            "iter 66600 step 23036: loss 4.1361, LR: 0.000200, iter time: 587.44ms\n",
            "iter 66700 step 23069: loss 3.8699, LR: 0.000195, iter time: 585.66ms\n",
            "iter 66800 step 23103: loss 4.0401, LR: 0.000191, iter time: 616.91ms (optimizer.step)\n",
            "iter 66900 step 23136: loss 3.5445, LR: 0.000186, iter time: 591.89ms\n",
            "iter 67000 step 23169: loss 2.8474, LR: 0.000182, iter time: 561.63ms\n",
            "iter 67100 step 23203: loss 3.3012, LR: 0.000178, iter time: 616.53ms (optimizer.step)\n",
            "iter 67200 step 23236: loss 3.9650, LR: 0.000173, iter time: 588.27ms\n",
            "iter 67300 step 23269: loss 3.5572, LR: 0.000169, iter time: 579.36ms\n",
            "iter 67400 step 23303: loss 3.9690, LR: 0.000165, iter time: 615.73ms (optimizer.step)\n",
            "iter 67500 step 23336: loss 3.8645, LR: 0.000161, iter time: 594.76ms\n",
            "iter 67600 step 23369: loss 3.9103, LR: 0.000157, iter time: 585.25ms\n",
            "iter 67700 step 23403: loss 3.1909, LR: 0.000153, iter time: 611.91ms (optimizer.step)\n",
            "iter 67800 step 23436: loss 4.0211, LR: 0.000149, iter time: 590.92ms\n",
            "iter 67900 step 23469: loss 3.9060, LR: 0.000145, iter time: 579.12ms\n",
            "iter 68000 step 23503: loss 3.6642, LR: 0.000141, iter time: 613.86ms (optimizer.step)\n",
            "iter 68100 step 23536: loss 3.3119, LR: 0.000137, iter time: 584.98ms\n",
            "iter 68200 step 23569: loss 3.9311, LR: 0.000133, iter time: 583.04ms\n",
            "iter 68300 step 23603: loss 3.8405, LR: 0.000130, iter time: 608.65ms (optimizer.step)\n",
            "iter 68400 step 23636: loss 3.2696, LR: 0.000126, iter time: 584.97ms\n",
            "iter 68500 step 23669: loss 4.1213, LR: 0.000122, iter time: 584.27ms\n",
            "iter 68600 step 23703: loss 3.0385, LR: 0.000119, iter time: 618.32ms (optimizer.step)\n",
            "iter 68700 step 23736: loss 3.4852, LR: 0.000115, iter time: 590.20ms\n",
            "iter 68800 step 23769: loss 3.1362, LR: 0.000112, iter time: 584.19ms\n",
            "iter 68900 step 23803: loss 3.8450, LR: 0.000109, iter time: 610.72ms (optimizer.step)\n",
            "iter 69000 step 23836: loss 3.8874, LR: 0.000105, iter time: 592.39ms\n",
            "iter 69100 step 23869: loss 3.6090, LR: 0.000102, iter time: 577.38ms\n",
            "iter 69200 step 23903: loss 2.9503, LR: 0.000099, iter time: 613.55ms (optimizer.step)\n",
            "iter 69300 step 23936: loss 3.1334, LR: 0.000096, iter time: 592.62ms\n",
            "iter 69400 step 23969: loss 3.4547, LR: 0.000093, iter time: 577.90ms\n",
            "69491 - Saving checkpoint to '/content/out/redpajama/version_1/iter-069491-ckpt.pth'\n",
            "iter 69491 step 24000: loss 3.3833, LR: 0.000090, iter time:\n",
            "iter 69500 step 24003: loss 3.7016, LR: 0.000090, iter time: 601.46ms (optimizer.step)\n",
            "iter 69600 step 24036: loss 3.7585, LR: 0.000087, iter time: 589.51ms\n",
            "iter 69700 step 24069: loss 3.4865, LR: 0.000084, iter time: 585.73ms\n",
            "iter 69800 step 24103: loss 3.5608, LR: 0.000081, iter time: 610.31ms (optimizer.step)\n",
            "iter 69900 step 24136: loss 3.1280, LR: 0.000078, iter time: 591.77ms\n",
            "iter 70000 step 24169: loss 3.6131, LR: 0.000075, iter time: 576.08ms\n",
            "iter 70100 step 24203: loss 4.0830, LR: 0.000072, iter time: 613.58ms (optimizer.step)\n",
            "iter 70200 step 24236: loss 3.8528, LR: 0.000070, iter time: 594.57ms\n",
            "iter 70300 step 24269: loss 3.9855, LR: 0.000067, iter time: 579.34ms\n",
            "iter 70400 step 24303: loss 3.7922, LR: 0.000065, iter time: 613.98ms (optimizer.step)\n",
            "iter 70500 step 24336: loss 3.8300, LR: 0.000062, iter time: 591.19ms\n",
            "iter 70600 step 24369: loss 3.7980, LR: 0.000060, iter time: 582.34ms\n",
            "iter 70700 step 24403: loss 3.6717, LR: 0.000057, iter time: 618.37ms (optimizer.step)\n",
            "iter 70800 step 24436: loss 3.9497, LR: 0.000055, iter time: 594.71ms\n",
            "iter 70900 step 24469: loss 3.8925, LR: 0.000053, iter time: 581.23ms\n",
            "iter 71000 step 24503: loss 3.8675, LR: 0.000050, iter time: 619.25ms (optimizer.step)\n",
            "iter 71100 step 24536: loss 3.8173, LR: 0.000048, iter time: 593.95ms\n",
            "iter 71200 step 24569: loss 3.8131, LR: 0.000046, iter time: 580.47ms\n",
            "iter 71300 step 24603: loss 3.7630, LR: 0.000044, iter time: 618.11ms (optimizer.step)\n",
            "iter 71400 step 24636: loss 4.3452, LR: 0.000042, iter time: 593.27ms\n",
            "iter 71500 step 24669: loss 3.6604, LR: 0.000040, iter time: 583.11ms\n",
            "iter 71600 step 24703: loss 3.8410, LR: 0.000038, iter time: 617.34ms (optimizer.step)\n",
            "iter 71700 step 24736: loss 3.4788, LR: 0.000036, iter time: 592.28ms\n",
            "iter 71800 step 24769: loss 3.8561, LR: 0.000034, iter time: 578.04ms\n",
            "iter 71900 step 24803: loss 3.5260, LR: 0.000033, iter time: 617.76ms (optimizer.step)\n",
            "iter 72000 step 24836: loss 3.9039, LR: 0.000031, iter time: 591.47ms\n",
            "iter 72100 step 24869: loss 3.6642, LR: 0.000029, iter time: 587.13ms\n",
            "iter 72200 step 24903: loss 3.5643, LR: 0.000028, iter time: 610.29ms (optimizer.step)\n",
            "iter 72300 step 24936: loss 4.0115, LR: 0.000026, iter time: 592.59ms\n",
            "iter 72400 step 24969: loss 4.0533, LR: 0.000025, iter time: 580.99ms\n",
            "72491 - Saving checkpoint to '/content/out/redpajama/version_1/iter-072491-ckpt.pth'\n",
            "iter 72491 step 25000: loss 4.2792, LR: 0.000023, iter time:\n",
            "iter 72500 step 25003: loss 3.7132, LR: 0.000023, iter time: 606.04ms (optimizer.step)\n",
            "iter 72600 step 25036: loss 2.8641, LR: 0.000022, iter time: 559.09ms\n",
            "iter 72700 step 25069: loss 3.6987, LR: 0.000021, iter time: 580.81ms\n",
            "iter 72800 step 25103: loss 3.8074, LR: 0.000019, iter time: 616.00ms (optimizer.step)\n",
            "iter 72900 step 25136: loss 3.4244, LR: 0.000018, iter time: 591.27ms\n",
            "iter 73000 step 25169: loss 3.9380, LR: 0.000017, iter time: 581.44ms\n",
            "iter 73100 step 25203: loss 3.9208, LR: 0.000016, iter time: 610.58ms (optimizer.step)\n",
            "iter 73200 step 25236: loss 3.7947, LR: 0.000015, iter time: 591.55ms\n",
            "iter 73300 step 25269: loss 3.8583, LR: 0.000014, iter time: 583.15ms\n",
            "iter 73400 step 25303: loss 3.6222, LR: 0.000013, iter time: 610.56ms (optimizer.step)\n",
            "iter 73500 step 25336: loss 4.0087, LR: 0.000012, iter time: 595.25ms\n",
            "iter 73600 step 25369: loss 4.0624, LR: 0.000011, iter time: 583.54ms\n",
            "iter 73700 step 25403: loss 3.6625, LR: 0.000011, iter time: 615.52ms (optimizer.step)\n",
            "iter 73800 step 25436: loss 3.7267, LR: 0.000010, iter time: 592.31ms\n",
            "iter 73900 step 25469: loss 3.5005, LR: 0.000009, iter time: 579.41ms\n",
            "iter 74000 step 25503: loss 3.8271, LR: 0.000009, iter time: 614.74ms (optimizer.step)\n",
            "iter 74100 step 25536: loss 3.7591, LR: 0.000008, iter time: 590.83ms\n",
            "iter 74200 step 25569: loss 3.5689, LR: 0.000008, iter time: 576.47ms\n",
            "iter 74300 step 25603: loss 3.5270, LR: 0.000007, iter time: 617.02ms (optimizer.step)\n",
            "iter 74400 step 25636: loss 3.7738, LR: 0.000007, iter time: 593.50ms\n",
            "iter 74500 step 25669: loss 3.4848, LR: 0.000007, iter time: 582.12ms\n",
            "iter 74600 step 25703: loss 3.6499, LR: 0.000006, iter time: 613.18ms (optimizer.step)\n",
            "iter 74700 step 25736: loss 3.4983, LR: 0.000006, iter time: 596.53ms\n",
            "iter 74800 step 25769: loss 3.7100, LR: 0.000006, iter time: 580.31ms\n",
            "iter 74900 step 25803: loss 3.2755, LR: 0.000006, iter time: 611.02ms (optimizer.step)\n",
            "iter 75000 step 25836: loss 2.6893\n",
            "Saving checkpoint to '/content/out/redpajama/version_1/iter-075000-ckpt.pth'\n",
            "Training time: 6031.07s\n",
            "Memory used: 12.24 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "setup(\n",
        "    devices=1,\n",
        "    train_data_dir=Path(\"data/lit-redpajama-sample\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8guCY37oiF0H",
        "outputId": "6c0d77a1-d4a2-4601-b578-e4c0e65f6300"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Using 16-bit Automatic Mixed Precision (AMP)\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using 16-bit Automatic Mixed Precision (AMP)\n",
            "INFO: Seed set to 1337\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 1337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "auto\n",
            "{'model_name': 'pythia-160m', 'name': 'redpajama', 'save_interval': 1000, 'eval_interval': 1000, 'eval_iters': 100, 'log_interval': 100, 'learning_rate': 0.006, 'batch_size': 8, 'micro_batch_size': 2, 'gradient_accumulation_steps': 4, 'max_iters': 65000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 2000, 'lr_decay_iters': 65000, 'min_lr': 6e-06}\n",
            "Loading model with {'name': 'pythia-160m', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
            "Time to instantiate model: 0.03 seconds.\n",
            "Total parameters 162,322,944\n",
            "Resume is true\n",
            "Resuming training from /content/out/redpajama/version_1/iter-058994-ckpt.pth\n",
            "Estimated TFLOPs: 5.54\n",
            "Measured TFLOPs: 3.96\n",
            "iter 59000 step 21002: loss 2.9293, LR: 0.000139, iter time: 286.80ms\n",
            "iter 59100 step 21027: loss 4.0077, LR: 0.000135, iter time: 310.22ms\n",
            "iter 59200 step 21052: loss 3.7533, LR: 0.000130, iter time: 303.14ms\n",
            "iter 59300 step 21077: loss 3.7975, LR: 0.000126, iter time: 305.45ms\n",
            "iter 59400 step 21102: loss 3.6598, LR: 0.000122, iter time: 299.68ms\n",
            "iter 59500 step 21127: loss 3.6804, LR: 0.000118, iter time: 305.92ms\n",
            "iter 59600 step 21152: loss 3.9962, LR: 0.000114, iter time: 305.45ms\n",
            "iter 59700 step 21177: loss 2.7709, LR: 0.000110, iter time: 303.57ms\n",
            "iter 59800 step 21202: loss 3.7465, LR: 0.000106, iter time: 305.22ms\n",
            "iter 59900 step 21227: loss 3.7261, LR: 0.000102, iter time: 306.60ms\n",
            "iter 60000 step 21252: loss 3.6770, LR: 0.000099, iter time: 310.71ms\n",
            "iter 60100 step 21277: loss 3.6695, LR: 0.000095, iter time: 302.39ms\n",
            "iter 60200 step 21302: loss 4.0882, LR: 0.000091, iter time: 307.65ms\n",
            "iter 60300 step 21327: loss 3.8004, LR: 0.000088, iter time: 306.57ms\n",
            "iter 60400 step 21352: loss 3.9603, LR: 0.000085, iter time: 307.21ms\n",
            "iter 60500 step 21377: loss 3.8838, LR: 0.000081, iter time: 303.72ms\n",
            "iter 60600 step 21402: loss 3.6662, LR: 0.000078, iter time: 302.18ms\n",
            "iter 60700 step 21427: loss 3.2605, LR: 0.000075, iter time: 308.20ms\n",
            "iter 60800 step 21452: loss 3.0027, LR: 0.000071, iter time: 304.60ms\n",
            "iter 60900 step 21477: loss 2.7747, LR: 0.000068, iter time: 307.52ms\n",
            "iter 61000 step 21502: loss 3.9400, LR: 0.000065, iter time: 308.34ms\n",
            "iter 61100 step 21527: loss 3.5511, LR: 0.000062, iter time: 304.92ms\n",
            "iter 61200 step 21552: loss 3.6036, LR: 0.000060, iter time: 304.46ms\n",
            "iter 61300 step 21577: loss 3.5458, LR: 0.000057, iter time: 306.55ms\n",
            "iter 61400 step 21602: loss 3.8741, LR: 0.000054, iter time: 306.23ms\n",
            "iter 61500 step 21627: loss 3.8995, LR: 0.000052, iter time: 305.47ms\n",
            "iter 61600 step 21652: loss 3.8767, LR: 0.000049, iter time: 309.35ms\n",
            "iter 61700 step 21677: loss 3.9476, LR: 0.000046, iter time: 306.77ms\n",
            "iter 61800 step 21702: loss 3.5534, LR: 0.000044, iter time: 305.28ms\n",
            "iter 61900 step 21727: loss 3.6323, LR: 0.000042, iter time: 306.13ms\n",
            "iter 62000 step 21752: loss 3.8778, LR: 0.000039, iter time: 306.37ms\n",
            "iter 62100 step 21777: loss 3.9597, LR: 0.000037, iter time: 306.16ms\n",
            "iter 62200 step 21802: loss 3.9957, LR: 0.000035, iter time: 305.12ms\n",
            "iter 62300 step 21827: loss 3.9968, LR: 0.000033, iter time: 307.95ms\n",
            "iter 62400 step 21852: loss 3.4228, LR: 0.000031, iter time: 308.02ms\n",
            "iter 62500 step 21877: loss 4.7061, LR: 0.000029, iter time: 308.61ms\n",
            "iter 62600 step 21902: loss 3.8281, LR: 0.000027, iter time: 305.95ms\n",
            "iter 62700 step 21927: loss 3.8747, LR: 0.000026, iter time: 304.27ms\n",
            "iter 62800 step 21952: loss 1.8700, LR: 0.000024, iter time: 288.03ms\n",
            "iter 62900 step 21977: loss 3.7885, LR: 0.000022, iter time: 307.03ms\n",
            "62991 - Saving checkpoint to '/content/out/redpajama/version_1/iter-062991-ckpt.pth'\n",
            "iter 62991 step 22000: loss 3.4720, LR: 0.000021, iter time:\n",
            "iter 63000 step 22002: loss 3.7265, LR: 0.000021, iter time: 296.25ms\n",
            "iter 63100 step 22027: loss 2.9645, LR: 0.000019, iter time: 317.15ms\n",
            "iter 63200 step 22052: loss 4.0655, LR: 0.000018, iter time: 304.63ms\n",
            "iter 63300 step 22077: loss 3.7726, LR: 0.000017, iter time: 312.59ms\n",
            "iter 63400 step 22102: loss 3.6226, LR: 0.000016, iter time: 307.31ms\n",
            "iter 63500 step 22127: loss 3.5457, LR: 0.000014, iter time: 303.29ms\n",
            "iter 63600 step 22152: loss 3.5423, LR: 0.000013, iter time: 309.58ms\n",
            "iter 63700 step 22177: loss 3.4203, LR: 0.000012, iter time: 305.30ms\n",
            "iter 63800 step 22202: loss 3.8980, LR: 0.000011, iter time: 307.10ms\n",
            "iter 63900 step 22227: loss 4.0297, LR: 0.000011, iter time: 305.07ms\n",
            "iter 64000 step 22252: loss 3.9667, LR: 0.000010, iter time: 305.66ms\n",
            "iter 64100 step 22277: loss 2.5420, LR: 0.000009, iter time: 306.48ms\n",
            "iter 64200 step 22302: loss 3.4239, LR: 0.000008, iter time: 305.21ms\n",
            "iter 64300 step 22327: loss 3.7883, LR: 0.000008, iter time: 308.17ms\n",
            "iter 64400 step 22352: loss 3.8060, LR: 0.000007, iter time: 310.01ms\n",
            "iter 64500 step 22377: loss 3.6026, LR: 0.000007, iter time: 309.47ms\n",
            "iter 64600 step 22402: loss 3.6456, LR: 0.000007, iter time: 304.59ms\n",
            "iter 64700 step 22427: loss 3.3925, LR: 0.000006, iter time: 303.60ms\n",
            "iter 64800 step 22452: loss 3.2008, LR: 0.000006, iter time: 308.70ms\n",
            "iter 64900 step 22477: loss 3.4466, LR: 0.000006, iter time: 304.24ms\n",
            "iter 65000 step 22502: loss 3.6228\n",
            "Saving checkpoint to '/content/out/redpajama/version_1/iter-065000-ckpt.pth'\n",
            "Training time: 1861.80s\n",
            "Memory used: 7.56 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "setup(\n",
        "    devices=1,\n",
        "    train_data_dir=Path(\"data/lit-redpajama-sample\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKHnJ0upH9kc",
        "outputId": "cb7b0aff-8ba1-4357-8c26-3b1fa4bf7492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Using 16-bit Automatic Mixed Precision (AMP)\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using 16-bit Automatic Mixed Precision (AMP)\n",
            "INFO: Seed set to 1337\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 1337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "auto\n",
            "{'model_name': 'pythia-160m', 'name': 'redpajama', 'save_interval': 1000, 'eval_interval': 1000, 'eval_iters': 100, 'log_interval': 100, 'learning_rate': 0.006, 'batch_size': 4, 'micro_batch_size': 1, 'gradient_accumulation_steps': 4, 'max_iters': 50000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 2000, 'lr_decay_iters': 50000, 'min_lr': 6e-06}\n",
            "Loading model with {'name': 'pythia-160m', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
            "Time to instantiate model: 0.02 seconds.\n",
            "Total parameters 162,322,944\n",
            "Resume is true\n",
            "Resuming training from /content/out/redpajama/version_1/iter-021997-ckpt.pth\n",
            "Estimated TFLOPs: 2.77\n",
            "Measured TFLOPs: 1.98\n",
            "iter 22000 step 11001: loss 4.6781, LR: 0.003779, iter time: 51.26ms\n",
            "iter 22100 step 11026: loss 5.3333, LR: 0.003760, iter time: 55.41ms\n",
            "iter 22200 step 11051: loss 4.7585, LR: 0.003741, iter time: 62.05ms\n",
            "iter 22300 step 11076: loss 5.3288, LR: 0.003722, iter time: 51.89ms\n",
            "iter 22400 step 11101: loss 4.9214, LR: 0.003703, iter time: 48.70ms\n",
            "iter 22500 step 11126: loss 5.0989, LR: 0.003684, iter time: 51.43ms\n",
            "iter 22600 step 11151: loss 5.3168, LR: 0.003664, iter time: 53.22ms\n",
            "iter 22700 step 11176: loss 5.0729, LR: 0.003645, iter time: 51.28ms\n",
            "iter 22800 step 11201: loss 5.1653, LR: 0.003626, iter time: 52.89ms\n",
            "iter 22900 step 11226: loss 5.2883, LR: 0.003607, iter time: 52.75ms\n",
            "iter 23000 step 11251: loss 7.1254, LR: 0.003588, iter time: 48.10ms\n",
            "iter 23100 step 11276: loss 5.2861, LR: 0.003568, iter time: 53.47ms\n",
            "iter 23200 step 11301: loss 5.1108, LR: 0.003549, iter time: 52.80ms\n",
            "iter 23300 step 11326: loss 5.4739, LR: 0.003530, iter time: 54.80ms\n",
            "iter 23400 step 11351: loss 4.9829, LR: 0.003511, iter time: 52.47ms\n",
            "iter 23500 step 11376: loss 4.8183, LR: 0.003491, iter time: 53.08ms\n",
            "iter 23600 step 11401: loss 6.1013, LR: 0.003472, iter time: 49.02ms\n",
            "iter 23700 step 11426: loss 6.0788, LR: 0.003452, iter time: 50.27ms\n",
            "iter 23800 step 11451: loss 5.0266, LR: 0.003433, iter time: 53.70ms\n",
            "iter 23900 step 11476: loss 4.6236, LR: 0.003414, iter time: 53.32ms\n",
            "iter 24000 step 11501: loss 4.9781, LR: 0.003394, iter time: 62.67ms\n",
            "iter 24100 step 11526: loss 5.1776, LR: 0.003375, iter time: 52.95ms\n",
            "iter 24200 step 11551: loss 4.7242, LR: 0.003355, iter time: 53.77ms\n",
            "iter 24300 step 11576: loss 4.9693, LR: 0.003336, iter time: 52.48ms\n",
            "iter 24400 step 11601: loss 4.9085, LR: 0.003316, iter time: 53.53ms\n",
            "iter 24500 step 11626: loss 5.0458, LR: 0.003297, iter time: 52.73ms\n",
            "iter 24600 step 11651: loss 4.6608, LR: 0.003277, iter time: 53.58ms\n",
            "iter 24700 step 11676: loss 5.1648, LR: 0.003258, iter time: 49.74ms\n",
            "iter 24800 step 11701: loss 4.6366, LR: 0.003238, iter time: 53.15ms\n",
            "iter 24900 step 11726: loss 4.8232, LR: 0.003219, iter time: 54.23ms\n",
            "iter 25000 step 11751: loss 5.0315, LR: 0.003199, iter time: 53.47ms\n",
            "iter 25100 step 11776: loss 4.8716, LR: 0.003179, iter time: 53.20ms\n",
            "iter 25200 step 11801: loss 4.9528, LR: 0.003160, iter time: 53.65ms\n",
            "iter 25300 step 11826: loss 4.4411, LR: 0.003140, iter time: 53.77ms\n",
            "iter 25400 step 11851: loss 4.8062, LR: 0.003121, iter time: 53.49ms\n",
            "iter 25500 step 11876: loss 4.8076, LR: 0.003101, iter time: 54.44ms\n",
            "iter 25600 step 11901: loss 4.9713, LR: 0.003081, iter time: 53.87ms\n",
            "iter 25700 step 11926: loss 4.8497, LR: 0.003062, iter time: 53.01ms\n",
            "iter 25800 step 11951: loss 5.1659, LR: 0.003042, iter time: 62.33ms\n",
            "iter 25900 step 11976: loss 4.7389, LR: 0.003023, iter time: 53.56ms\n",
            "25995 - Saving checkpoint to '/content/out/redpajama/version_1/iter-025995-ckpt.pth'\n",
            "iter 25995 step 12000: loss 4.8094, LR: 0.003004, iter time:\n",
            "iter 26000 step 12001: loss 5.0813, LR: 0.003003, iter time: 51.82ms\n",
            "iter 26100 step 12026: loss 5.0365, LR: 0.002983, iter time: 50.82ms\n",
            "iter 26200 step 12051: loss 6.4073, LR: 0.002964, iter time: 54.68ms\n",
            "iter 26300 step 12076: loss 4.9193, LR: 0.002944, iter time: 50.68ms\n",
            "iter 26400 step 12101: loss 4.2997, LR: 0.002925, iter time: 53.30ms\n",
            "iter 26500 step 12126: loss 4.8188, LR: 0.002905, iter time: 53.34ms\n",
            "iter 26600 step 12151: loss 6.4302, LR: 0.002885, iter time: 49.24ms\n",
            "iter 26700 step 12176: loss 4.7554, LR: 0.002866, iter time: 54.57ms\n",
            "iter 26800 step 12201: loss 6.6851, LR: 0.002846, iter time: 52.87ms\n",
            "iter 26900 step 12226: loss 4.8249, LR: 0.002827, iter time: 51.99ms\n",
            "iter 27000 step 12251: loss 4.8791, LR: 0.002807, iter time: 52.97ms\n",
            "iter 27100 step 12276: loss 5.0726, LR: 0.002787, iter time: 53.03ms\n",
            "iter 27200 step 12301: loss 4.9066, LR: 0.002768, iter time: 51.59ms\n",
            "iter 27300 step 12326: loss 4.4291, LR: 0.002748, iter time: 50.87ms\n",
            "iter 27400 step 12351: loss 7.2724, LR: 0.002729, iter time: 49.79ms\n",
            "iter 27500 step 12376: loss 5.0801, LR: 0.002709, iter time: 59.19ms\n",
            "iter 27600 step 12401: loss 4.5780, LR: 0.002690, iter time: 53.45ms\n",
            "iter 27700 step 12426: loss 4.8684, LR: 0.002670, iter time: 52.81ms\n",
            "iter 27800 step 12451: loss 5.2979, LR: 0.002651, iter time: 54.60ms\n",
            "iter 27900 step 12476: loss 4.5318, LR: 0.002631, iter time: 54.41ms\n",
            "iter 28000 step 12501: loss 4.7219, LR: 0.002612, iter time: 52.47ms\n",
            "iter 28100 step 12526: loss 4.6136, LR: 0.002592, iter time: 57.28ms\n",
            "iter 28200 step 12551: loss 4.7609, LR: 0.002573, iter time: 53.20ms\n",
            "iter 28300 step 12576: loss 3.1339, LR: 0.002554, iter time: 52.82ms\n",
            "iter 28400 step 12601: loss 4.8324, LR: 0.002534, iter time: 53.45ms\n",
            "iter 28500 step 12626: loss 4.6696, LR: 0.002515, iter time: 54.16ms\n",
            "iter 28600 step 12651: loss 4.0273, LR: 0.002495, iter time: 53.13ms\n",
            "iter 28700 step 12676: loss 4.9429, LR: 0.002476, iter time: 53.43ms\n",
            "iter 28800 step 12701: loss 4.8071, LR: 0.002457, iter time: 53.42ms\n",
            "iter 28900 step 12726: loss 5.0547, LR: 0.002438, iter time: 54.16ms\n",
            "iter 29000 step 12751: loss 4.3895, LR: 0.002418, iter time: 53.84ms\n",
            "iter 29100 step 12776: loss 4.6496, LR: 0.002399, iter time: 52.38ms\n",
            "iter 29200 step 12801: loss 5.3595, LR: 0.002380, iter time: 52.76ms\n",
            "iter 29300 step 12826: loss 4.8057, LR: 0.002361, iter time: 54.23ms\n",
            "iter 29400 step 12851: loss 4.7562, LR: 0.002342, iter time: 53.87ms\n",
            "iter 29500 step 12876: loss 4.6884, LR: 0.002322, iter time: 53.16ms\n",
            "iter 29600 step 12901: loss 4.8422, LR: 0.002303, iter time: 53.79ms\n",
            "iter 29700 step 12926: loss 4.9555, LR: 0.002284, iter time: 52.49ms\n",
            "iter 29800 step 12951: loss 2.4970, LR: 0.002265, iter time: 57.29ms\n",
            "iter 29900 step 12976: loss 4.1730, LR: 0.002246, iter time: 53.67ms\n",
            "29995 - Saving checkpoint to '/content/out/redpajama/version_1/iter-029995-ckpt.pth'\n",
            "iter 29995 step 13000: loss 4.6043, LR: 0.002228, iter time:\n",
            "iter 30000 step 13001: loss 4.8620, LR: 0.002227, iter time: 49.03ms\n",
            "iter 30100 step 13026: loss 4.9881, LR: 0.002208, iter time: 53.18ms\n",
            "iter 30200 step 13051: loss 4.9328, LR: 0.002189, iter time: 58.75ms\n",
            "iter 30300 step 13076: loss 4.1564, LR: 0.002171, iter time: 54.19ms\n",
            "iter 30400 step 13101: loss 2.4692, LR: 0.002152, iter time: 53.11ms\n",
            "iter 30500 step 13126: loss 4.5350, LR: 0.002133, iter time: 53.09ms\n",
            "iter 30600 step 13151: loss 4.5229, LR: 0.002114, iter time: 53.53ms\n",
            "iter 30700 step 13176: loss 5.1567, LR: 0.002096, iter time: 52.92ms\n",
            "iter 30800 step 13201: loss 4.5468, LR: 0.002077, iter time: 51.73ms\n",
            "iter 30900 step 13226: loss 4.5571, LR: 0.002058, iter time: 53.70ms\n",
            "iter 31000 step 13251: loss 5.0544, LR: 0.002040, iter time: 53.74ms\n",
            "iter 31100 step 13276: loss 4.6718, LR: 0.002021, iter time: 54.44ms\n",
            "iter 31200 step 13301: loss 4.1839, LR: 0.002003, iter time: 53.04ms\n",
            "iter 31300 step 13326: loss 4.4090, LR: 0.001984, iter time: 52.73ms\n",
            "iter 31400 step 13351: loss 4.5148, LR: 0.001966, iter time: 53.85ms\n",
            "iter 31500 step 13376: loss 4.4575, LR: 0.001947, iter time: 53.68ms\n",
            "iter 31600 step 13401: loss 4.8250, LR: 0.001929, iter time: 52.93ms\n",
            "iter 31700 step 13426: loss 4.9325, LR: 0.001911, iter time: 53.44ms\n",
            "iter 31800 step 13451: loss 4.6118, LR: 0.001892, iter time: 59.53ms\n",
            "iter 31900 step 13476: loss 4.7149, LR: 0.001874, iter time: 56.53ms\n",
            "iter 32000 step 13501: loss 4.7871, LR: 0.001856, iter time: 53.47ms\n",
            "iter 32100 step 13526: loss 4.6344, LR: 0.001838, iter time: 53.22ms\n",
            "iter 32200 step 13551: loss 4.6901, LR: 0.001820, iter time: 52.92ms\n",
            "iter 32300 step 13576: loss 4.6605, LR: 0.001802, iter time: 53.65ms\n",
            "iter 32400 step 13601: loss 4.3683, LR: 0.001784, iter time: 53.88ms\n",
            "iter 32500 step 13626: loss 4.3791, LR: 0.001766, iter time: 54.46ms\n",
            "iter 32600 step 13651: loss 4.1949, LR: 0.001748, iter time: 52.93ms\n",
            "iter 32700 step 13676: loss 4.7698, LR: 0.001730, iter time: 54.05ms\n",
            "iter 32800 step 13701: loss 4.9145, LR: 0.001713, iter time: 53.05ms\n",
            "iter 32900 step 13726: loss 4.5139, LR: 0.001695, iter time: 59.28ms\n",
            "iter 33000 step 13751: loss 4.2899, LR: 0.001677, iter time: 53.72ms\n",
            "iter 33100 step 13776: loss 3.9925, LR: 0.001660, iter time: 54.39ms\n",
            "iter 33200 step 13801: loss 4.4021, LR: 0.001642, iter time: 54.29ms\n",
            "iter 33300 step 13826: loss 4.6236, LR: 0.001625, iter time: 53.64ms\n",
            "iter 33400 step 13851: loss 4.4513, LR: 0.001608, iter time: 53.65ms\n",
            "iter 33500 step 13876: loss 5.0962, LR: 0.001590, iter time: 53.64ms\n",
            "iter 33600 step 13901: loss 3.6974, LR: 0.001573, iter time: 56.08ms\n",
            "iter 33700 step 13926: loss 4.6915, LR: 0.001556, iter time: 54.04ms\n",
            "iter 33800 step 13951: loss 4.0691, LR: 0.001539, iter time: 54.27ms\n",
            "iter 33900 step 13976: loss 3.1089, LR: 0.001522, iter time: 52.89ms\n",
            "33995 - Saving checkpoint to '/content/out/redpajama/version_1/iter-033995-ckpt.pth'\n",
            "iter 33995 step 14000: loss 4.4536, LR: 0.001505, iter time:\n",
            "iter 34000 step 14001: loss 1.9098, LR: 0.001505, iter time: 52.31ms\n",
            "iter 34100 step 14026: loss 4.3780, LR: 0.001488, iter time: 53.44ms\n",
            "iter 34200 step 14051: loss 2.2775, LR: 0.001471, iter time: 53.03ms\n",
            "iter 34300 step 14076: loss 3.9691, LR: 0.001454, iter time: 53.78ms\n",
            "iter 34400 step 14101: loss 4.4978, LR: 0.001437, iter time: 50.94ms\n",
            "iter 34500 step 14126: loss 3.4070, LR: 0.001420, iter time: 49.70ms\n",
            "iter 34600 step 14151: loss 4.0423, LR: 0.001404, iter time: 51.05ms\n",
            "iter 34700 step 14176: loss 3.9088, LR: 0.001387, iter time: 54.64ms\n",
            "iter 34800 step 14201: loss 4.4006, LR: 0.001371, iter time: 53.76ms\n",
            "iter 34900 step 14226: loss 4.1055, LR: 0.001354, iter time: 53.21ms\n",
            "iter 35000 step 14251: loss 4.5652, LR: 0.001338, iter time: 53.55ms\n",
            "iter 35100 step 14276: loss 2.8419, LR: 0.001322, iter time: 53.64ms\n",
            "iter 35200 step 14301: loss 3.3818, LR: 0.001305, iter time: 56.48ms\n",
            "iter 35300 step 14326: loss 4.6522, LR: 0.001289, iter time: 53.51ms\n",
            "iter 35400 step 14351: loss 4.0371, LR: 0.001273, iter time: 53.78ms\n",
            "iter 35500 step 14376: loss 4.1339, LR: 0.001257, iter time: 53.43ms\n",
            "iter 35600 step 14401: loss 4.3528, LR: 0.001241, iter time: 53.54ms\n",
            "iter 35700 step 14426: loss 3.7557, LR: 0.001226, iter time: 52.37ms\n",
            "iter 35800 step 14451: loss 4.2845, LR: 0.001210, iter time: 54.52ms\n",
            "iter 35900 step 14476: loss 4.5747, LR: 0.001194, iter time: 53.69ms\n",
            "iter 36000 step 14501: loss 4.5672, LR: 0.001179, iter time: 53.82ms\n",
            "iter 36100 step 14526: loss 4.0775, LR: 0.001163, iter time: 54.27ms\n",
            "iter 36200 step 14551: loss 3.9821, LR: 0.001148, iter time: 52.55ms\n",
            "iter 36300 step 14576: loss 4.5718, LR: 0.001132, iter time: 59.82ms\n",
            "iter 36400 step 14601: loss 3.7814, LR: 0.001117, iter time: 54.75ms\n",
            "iter 36500 step 14626: loss 4.6541, LR: 0.001102, iter time: 54.03ms\n",
            "iter 36600 step 14651: loss 4.1955, LR: 0.001087, iter time: 53.97ms\n",
            "iter 36700 step 14676: loss 4.1630, LR: 0.001072, iter time: 53.83ms\n",
            "iter 36800 step 14701: loss 4.2525, LR: 0.001057, iter time: 53.20ms\n",
            "iter 36900 step 14726: loss 4.2717, LR: 0.001042, iter time: 54.41ms\n",
            "iter 37000 step 14751: loss 4.4901, LR: 0.001027, iter time: 53.46ms\n",
            "iter 37100 step 14776: loss 4.2798, LR: 0.001012, iter time: 53.54ms\n",
            "iter 37200 step 14801: loss 2.8778, LR: 0.000998, iter time: 52.54ms\n",
            "iter 37300 step 14826: loss 4.2304, LR: 0.000983, iter time: 52.43ms\n",
            "iter 37400 step 14851: loss 4.1195, LR: 0.000969, iter time: 52.41ms\n",
            "iter 37500 step 14876: loss 5.5738, LR: 0.000954, iter time: 49.44ms\n",
            "iter 37600 step 14901: loss 4.3449, LR: 0.000940, iter time: 53.95ms\n",
            "iter 37700 step 14926: loss 4.1682, LR: 0.000926, iter time: 53.81ms\n",
            "iter 37800 step 14951: loss 5.2745, LR: 0.000912, iter time: 51.40ms\n",
            "iter 37900 step 14976: loss 4.2631, LR: 0.000898, iter time: 53.51ms\n",
            "37995 - Saving checkpoint to '/content/out/redpajama/version_1/iter-037995-ckpt.pth'\n",
            "iter 37995 step 15000: loss 4.5966, LR: 0.000884, iter time:\n",
            "iter 38000 step 15001: loss 4.3153, LR: 0.000884, iter time: 52.91ms\n",
            "iter 38100 step 15026: loss 4.2807, LR: 0.000870, iter time: 54.24ms\n",
            "iter 38200 step 15051: loss 4.3050, LR: 0.000856, iter time: 53.89ms\n",
            "iter 38300 step 15076: loss 4.2385, LR: 0.000843, iter time: 53.53ms\n",
            "iter 38400 step 15101: loss 3.9789, LR: 0.000829, iter time: 53.08ms\n",
            "iter 38500 step 15126: loss 4.4647, LR: 0.000816, iter time: 52.89ms\n",
            "iter 38600 step 15151: loss 4.0408, LR: 0.000802, iter time: 54.39ms\n",
            "iter 38700 step 15176: loss 3.6236, LR: 0.000789, iter time: 56.87ms\n",
            "iter 38800 step 15201: loss 3.6549, LR: 0.000776, iter time: 51.93ms\n",
            "iter 38900 step 15226: loss 4.4445, LR: 0.000763, iter time: 53.57ms\n",
            "iter 39000 step 15251: loss 4.3620, LR: 0.000750, iter time: 53.13ms\n",
            "iter 39100 step 15276: loss 2.2334, LR: 0.000737, iter time: 54.24ms\n",
            "iter 39200 step 15301: loss 4.4742, LR: 0.000724, iter time: 53.25ms\n",
            "iter 39300 step 15326: loss 4.2473, LR: 0.000711, iter time: 54.37ms\n",
            "iter 39400 step 15351: loss 4.3816, LR: 0.000699, iter time: 53.89ms\n",
            "iter 39500 step 15376: loss 4.4828, LR: 0.000686, iter time: 53.31ms\n",
            "iter 39600 step 15401: loss 3.0996, LR: 0.000674, iter time: 53.51ms\n",
            "iter 39700 step 15426: loss 4.4730, LR: 0.000662, iter time: 53.12ms\n",
            "iter 39800 step 15451: loss 4.7077, LR: 0.000649, iter time: 63.40ms\n",
            "iter 39900 step 15476: loss 3.7500, LR: 0.000637, iter time: 54.17ms\n",
            "iter 40000 step 15501: loss 3.9750, LR: 0.000625, iter time: 53.81ms\n",
            "iter 40100 step 15526: loss 3.9000, LR: 0.000613, iter time: 53.70ms\n",
            "iter 40200 step 15551: loss 3.9332, LR: 0.000602, iter time: 53.92ms\n",
            "iter 40300 step 15576: loss 2.9228, LR: 0.000590, iter time: 53.33ms\n",
            "iter 40400 step 15601: loss 4.1659, LR: 0.000578, iter time: 53.36ms\n",
            "iter 40500 step 15626: loss 3.8108, LR: 0.000567, iter time: 53.96ms\n",
            "iter 40600 step 15651: loss 3.9794, LR: 0.000556, iter time: 53.77ms\n",
            "iter 40700 step 15676: loss 4.2208, LR: 0.000544, iter time: 52.60ms\n",
            "iter 40800 step 15701: loss 4.2779, LR: 0.000533, iter time: 52.81ms\n",
            "iter 40900 step 15726: loss 4.2395, LR: 0.000522, iter time: 53.46ms\n",
            "iter 41000 step 15751: loss 4.2741, LR: 0.000511, iter time: 52.80ms\n",
            "iter 41100 step 15776: loss 3.8279, LR: 0.000500, iter time: 53.18ms\n",
            "iter 41200 step 15801: loss 4.0090, LR: 0.000490, iter time: 53.09ms\n",
            "iter 41300 step 15826: loss 4.1089, LR: 0.000479, iter time: 53.72ms\n",
            "iter 41400 step 15851: loss 0.7676, LR: 0.000468, iter time: 53.81ms\n",
            "iter 41500 step 15876: loss 4.1366, LR: 0.000458, iter time: 53.82ms\n",
            "iter 41600 step 15901: loss 3.9702, LR: 0.000448, iter time: 54.78ms\n",
            "iter 41700 step 15926: loss 3.7680, LR: 0.000437, iter time: 54.07ms\n",
            "iter 41800 step 15951: loss 4.0213, LR: 0.000427, iter time: 53.56ms\n",
            "iter 41900 step 15976: loss 4.2214, LR: 0.000417, iter time: 52.90ms\n",
            "41995 - Saving checkpoint to '/content/out/redpajama/version_1/iter-041995-ckpt.pth'\n",
            "iter 41995 step 16000: loss 4.5099, LR: 0.000408, iter time:\n",
            "iter 42000 step 16001: loss 0.5991, LR: 0.000408, iter time: 49.96ms\n",
            "iter 42100 step 16026: loss 4.3725, LR: 0.000398, iter time: 51.07ms\n",
            "iter 42200 step 16051: loss 4.2631, LR: 0.000388, iter time: 51.38ms\n",
            "iter 42300 step 16076: loss 3.7328, LR: 0.000379, iter time: 51.81ms\n",
            "iter 42400 step 16101: loss 4.3346, LR: 0.000369, iter time: 53.59ms\n",
            "iter 42500 step 16126: loss 3.6438, LR: 0.000360, iter time: 54.06ms\n",
            "iter 42600 step 16151: loss 4.1992, LR: 0.000351, iter time: 53.83ms\n",
            "iter 42700 step 16176: loss 4.5387, LR: 0.000342, iter time: 53.57ms\n",
            "iter 42800 step 16201: loss 3.6225, LR: 0.000333, iter time: 53.81ms\n",
            "iter 42900 step 16226: loss 3.7645, LR: 0.000324, iter time: 53.83ms\n",
            "iter 43000 step 16251: loss 6.0600, LR: 0.000315, iter time: 53.07ms\n",
            "iter 43100 step 16276: loss 3.9312, LR: 0.000306, iter time: 54.00ms\n",
            "iter 43200 step 16301: loss 4.5181, LR: 0.000298, iter time: 52.37ms\n",
            "iter 43300 step 16326: loss 2.2690, LR: 0.000290, iter time: 53.96ms\n",
            "iter 43400 step 16351: loss 4.1986, LR: 0.000281, iter time: 52.38ms\n",
            "iter 43500 step 16376: loss 4.5114, LR: 0.000273, iter time: 53.90ms\n",
            "iter 43600 step 16401: loss 3.7255, LR: 0.000265, iter time: 54.53ms\n",
            "iter 43700 step 16426: loss 4.0751, LR: 0.000257, iter time: 53.62ms\n",
            "iter 43800 step 16451: loss 4.2379, LR: 0.000249, iter time: 53.22ms\n",
            "iter 43900 step 16476: loss 4.4147, LR: 0.000242, iter time: 53.84ms\n",
            "iter 44000 step 16501: loss 4.1129, LR: 0.000234, iter time: 54.39ms\n",
            "iter 44100 step 16526: loss 4.1825, LR: 0.000227, iter time: 50.88ms\n",
            "iter 44200 step 16551: loss 4.2569, LR: 0.000219, iter time: 53.60ms\n",
            "iter 44300 step 16576: loss 4.4630, LR: 0.000212, iter time: 53.08ms\n",
            "iter 44400 step 16601: loss 4.0859, LR: 0.000205, iter time: 55.32ms\n",
            "iter 44500 step 16626: loss 3.6653, LR: 0.000198, iter time: 52.47ms\n",
            "iter 44600 step 16651: loss 4.3564, LR: 0.000191, iter time: 53.31ms\n",
            "iter 44700 step 16676: loss 2.1437, LR: 0.000185, iter time: 54.29ms\n",
            "iter 44800 step 16701: loss 3.3748, LR: 0.000178, iter time: 53.54ms\n",
            "iter 44900 step 16726: loss 4.1916, LR: 0.000171, iter time: 52.92ms\n",
            "iter 45000 step 16751: loss 4.3169, LR: 0.000165, iter time: 54.17ms\n",
            "iter 45100 step 16776: loss 3.5016, LR: 0.000159, iter time: 54.20ms\n",
            "iter 45200 step 16801: loss 3.3689, LR: 0.000153, iter time: 54.12ms\n",
            "iter 45300 step 16826: loss 4.0718, LR: 0.000147, iter time: 53.61ms\n",
            "iter 45400 step 16851: loss 3.8316, LR: 0.000141, iter time: 54.08ms\n",
            "iter 45500 step 16876: loss 4.2544, LR: 0.000135, iter time: 53.76ms\n",
            "iter 45600 step 16901: loss 3.7936, LR: 0.000129, iter time: 53.53ms\n",
            "iter 45700 step 16926: loss 3.9482, LR: 0.000124, iter time: 53.50ms\n",
            "iter 45800 step 16951: loss 3.7056, LR: 0.000119, iter time: 54.60ms\n",
            "iter 45900 step 16976: loss 4.1428, LR: 0.000113, iter time: 52.78ms\n",
            "45995 - Saving checkpoint to '/content/out/redpajama/version_1/iter-045995-ckpt.pth'\n",
            "iter 45995 step 17000: loss 3.8341, LR: 0.000108, iter time:\n",
            "iter 46000 step 17001: loss 4.0900, LR: 0.000108, iter time: 52.99ms\n",
            "iter 46100 step 17026: loss 2.0535, LR: 0.000103, iter time: 55.74ms\n",
            "iter 46200 step 17051: loss 2.7352, LR: 0.000098, iter time: 54.37ms\n",
            "iter 46300 step 17076: loss 4.0374, LR: 0.000093, iter time: 54.24ms\n",
            "iter 46400 step 17101: loss 4.1371, LR: 0.000089, iter time: 53.58ms\n",
            "iter 46500 step 17126: loss 4.1090, LR: 0.000084, iter time: 49.34ms\n",
            "iter 46600 step 17151: loss 3.0372, LR: 0.000080, iter time: 49.79ms\n",
            "iter 46700 step 17176: loss 3.6957, LR: 0.000076, iter time: 53.19ms\n",
            "iter 46800 step 17201: loss 3.7118, LR: 0.000071, iter time: 53.78ms\n",
            "iter 46900 step 17226: loss 4.2448, LR: 0.000067, iter time: 54.61ms\n",
            "iter 47000 step 17251: loss 0.0000, LR: 0.000064, iter time: 53.91ms\n",
            "iter 47100 step 17276: loss 5.2717, LR: 0.000060, iter time: 53.60ms\n",
            "iter 47200 step 17301: loss 3.7904, LR: 0.000056, iter time: 51.68ms\n",
            "iter 47300 step 17326: loss 4.2011, LR: 0.000053, iter time: 57.58ms\n",
            "iter 47400 step 17351: loss 4.1649, LR: 0.000049, iter time: 53.91ms\n",
            "iter 47500 step 17376: loss 4.3090, LR: 0.000046, iter time: 54.38ms\n",
            "iter 47600 step 17401: loss 4.0204, LR: 0.000043, iter time: 53.19ms\n",
            "iter 47700 step 17426: loss 2.3859, LR: 0.000040, iter time: 54.35ms\n",
            "iter 47800 step 17451: loss 3.9468, LR: 0.000037, iter time: 53.98ms\n",
            "iter 47900 step 17476: loss 0.0000, LR: 0.000034, iter time: 52.89ms\n",
            "iter 48000 step 17501: loss 3.8924, LR: 0.000032, iter time: 53.84ms\n",
            "iter 48100 step 17526: loss 4.1945, LR: 0.000029, iter time: 54.42ms\n",
            "iter 48200 step 17551: loss 3.2923, LR: 0.000027, iter time: 54.21ms\n",
            "iter 48300 step 17576: loss 4.0026, LR: 0.000025, iter time: 52.36ms\n",
            "iter 48400 step 17601: loss 4.2964, LR: 0.000022, iter time: 53.22ms\n",
            "iter 48500 step 17626: loss 4.1916, LR: 0.000020, iter time: 54.07ms\n",
            "iter 48600 step 17651: loss 4.0661, LR: 0.000019, iter time: 54.07ms\n",
            "iter 48700 step 17676: loss 4.1388, LR: 0.000017, iter time: 54.92ms\n",
            "iter 48800 step 17701: loss 3.9583, LR: 0.000015, iter time: 53.40ms\n",
            "iter 48900 step 17726: loss 3.8648, LR: 0.000014, iter time: 53.52ms\n",
            "iter 49000 step 17751: loss 3.7654, LR: 0.000012, iter time: 59.93ms\n",
            "iter 49100 step 17776: loss 4.3049, LR: 0.000011, iter time: 53.27ms\n",
            "iter 49200 step 17801: loss 3.4102, LR: 0.000010, iter time: 49.70ms\n",
            "iter 49300 step 17826: loss 4.1338, LR: 0.000009, iter time: 53.38ms\n",
            "iter 49400 step 17851: loss 4.2430, LR: 0.000008, iter time: 54.21ms\n",
            "iter 49500 step 17876: loss 4.1665, LR: 0.000008, iter time: 53.32ms\n",
            "iter 49600 step 17901: loss 4.1970, LR: 0.000007, iter time: 57.88ms\n",
            "iter 49700 step 17926: loss 2.5099, LR: 0.000007, iter time: 54.20ms\n",
            "iter 49800 step 17951: loss 4.2879, LR: 0.000006, iter time: 54.36ms\n",
            "iter 49900 step 17976: loss 4.0260, LR: 0.000006, iter time: 53.24ms\n",
            "49995 - Saving checkpoint to '/content/out/redpajama/version_1/iter-049995-ckpt.pth'\n",
            "iter 49995 step 18000: loss 2.8982, LR: 0.000006, iter time:\n",
            "iter 50000 step 18001: loss 4.0712\n",
            "Saving checkpoint to '/content/out/redpajama/version_1/iter-050000-ckpt.pth'\n",
            "Training time: 4936.28s\n",
            "Memory used: 7.23 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "setup(\n",
        "    devices=1,\n",
        "    train_data_dir=Path(\"data/lit-redpajama-sample\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOo29gBhknCQ",
        "outputId": "20110aa6-9709-4b18-93bc-1bd1c76ff36c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Using 16-bit Automatic Mixed Precision (AMP)\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using 16-bit Automatic Mixed Precision (AMP)\n",
            "INFO: Seed set to 1337\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 1337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "auto\n",
            "{'model_name': 'pythia-160m', 'name': 'redpajama', 'save_interval': 1000, 'eval_interval': 1000, 'eval_iters': 100, 'log_interval': 100, 'learning_rate': 0.006, 'batch_size': 2, 'micro_batch_size': 1, 'gradient_accumulation_steps': 2, 'max_iters': 16000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 2000, 'lr_decay_iters': 16000, 'min_lr': 6e-06}\n",
            "Loading model with {'name': 'pythia-160m', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
            "Time to instantiate model: 0.02 seconds.\n",
            "Total parameters 162,322,944\n",
            "Resume is true\n",
            "Resuming training from /content/out/redpajama/version_1/iter-016000-ckpt.pth\n",
            "Estimated TFLOPs: 2.77\n",
            "Measured TFLOPs: 1.98\n",
            "iter 16000 step 8001: loss 5.1993, LR: 0.001999, iter time: 47.62ms\n",
            "iter 16100 step 8051: loss 5.1279, LR: 0.001961, iter time: 49.71ms\n",
            "iter 16200 step 8101: loss 5.8033, LR: 0.001923, iter time: 49.83ms\n",
            "iter 16300 step 8151: loss 5.1348, LR: 0.001885, iter time: 50.51ms\n",
            "iter 16400 step 8201: loss 5.5314, LR: 0.001847, iter time: 49.84ms\n",
            "iter 16500 step 8251: loss 5.1398, LR: 0.001809, iter time: 50.74ms\n",
            "iter 16600 step 8301: loss 5.2470, LR: 0.001772, iter time: 47.61ms\n",
            "iter 16700 step 8351: loss 7.0150, LR: 0.001734, iter time: 50.68ms\n",
            "iter 16800 step 8401: loss 4.7558, LR: 0.001697, iter time: 50.11ms\n",
            "iter 16900 step 8451: loss 5.2358, LR: 0.001661, iter time: 50.63ms\n",
            "iter 17000 step 8501: loss 5.2756, LR: 0.001624, iter time: 53.77ms\n",
            "iter 17100 step 8551: loss 5.3944, LR: 0.001588, iter time: 51.54ms\n",
            "iter 17200 step 8601: loss 5.7512, LR: 0.001552, iter time: 50.46ms\n",
            "iter 17300 step 8651: loss 0.0000, LR: 0.001516, iter time: 47.42ms\n",
            "iter 17400 step 8701: loss 5.6132, LR: 0.001481, iter time: 50.65ms\n",
            "iter 17500 step 8751: loss 5.3112, LR: 0.001446, iter time: 49.79ms\n",
            "iter 17600 step 8801: loss 5.1889, LR: 0.001411, iter time: 51.10ms\n",
            "iter 17700 step 8851: loss 5.0383, LR: 0.001376, iter time: 50.66ms\n",
            "iter 17800 step 8901: loss 5.1971, LR: 0.001342, iter time: 50.59ms\n",
            "iter 17900 step 8951: loss 5.2564, LR: 0.001308, iter time: 49.61ms\n",
            "17997 - Saving checkpoint to '/content/out/redpajama/version_1/iter-017997-ckpt.pth'\n",
            "iter 17997 step 9000: loss 5.5413, LR: 0.001276, iter time:\n",
            "iter 18000 step 9001: loss 5.3207, LR: 0.001275, iter time: 49.87ms\n",
            "iter 18100 step 9051: loss 5.4142, LR: 0.001241, iter time: 50.04ms\n",
            "iter 18200 step 9101: loss 5.6650, LR: 0.001208, iter time: 51.68ms\n",
            "iter 18300 step 9151: loss 4.7883, LR: 0.001176, iter time: 50.44ms\n",
            "iter 18400 step 9201: loss 4.1800, LR: 0.001144, iter time: 48.68ms\n",
            "iter 18500 step 9251: loss 5.1032, LR: 0.001112, iter time: 50.01ms\n",
            "iter 18600 step 9301: loss 5.1619, LR: 0.001080, iter time: 49.17ms\n",
            "iter 18700 step 9351: loss 4.9655, LR: 0.001049, iter time: 54.08ms\n",
            "iter 18800 step 9401: loss 6.6459, LR: 0.001018, iter time: 49.50ms\n",
            "iter 18900 step 9451: loss 4.6267, LR: 0.000987, iter time: 50.59ms\n",
            "iter 19000 step 9501: loss 0.9887, LR: 0.000957, iter time: 49.73ms\n",
            "iter 19100 step 9551: loss 4.9960, LR: 0.000928, iter time: 48.29ms\n",
            "iter 19200 step 9601: loss 5.0617, LR: 0.000898, iter time: 53.33ms\n",
            "iter 19300 step 9651: loss 4.9181, LR: 0.000869, iter time: 49.22ms\n",
            "iter 19400 step 9701: loss 5.4181, LR: 0.000841, iter time: 50.47ms\n",
            "iter 19500 step 9751: loss 4.9089, LR: 0.000813, iter time: 48.39ms\n",
            "iter 19600 step 9801: loss 5.4823, LR: 0.000785, iter time: 47.44ms\n",
            "iter 19700 step 9851: loss 5.0629, LR: 0.000758, iter time: 57.08ms\n",
            "iter 19800 step 9901: loss 0.0000, LR: 0.000731, iter time: 47.68ms\n",
            "iter 19900 step 9951: loss 5.0229, LR: 0.000704, iter time: 50.06ms\n",
            "19997 - Saving checkpoint to '/content/out/redpajama/version_1/iter-019997-ckpt.pth'\n",
            "iter 19997 step 10000: loss 5.2993, LR: 0.000679, iter time:\n",
            "iter 20000 step 10001: loss 4.8747, LR: 0.000678, iter time: 46.57ms\n",
            "iter 20100 step 10051: loss 4.8388, LR: 0.000653, iter time: 49.82ms\n",
            "iter 20200 step 10101: loss 5.5882, LR: 0.000627, iter time: 48.54ms\n",
            "iter 20300 step 10151: loss 5.0160, LR: 0.000603, iter time: 50.49ms\n",
            "iter 20400 step 10201: loss 4.7181, LR: 0.000578, iter time: 50.18ms\n",
            "iter 20500 step 10251: loss 5.0845, LR: 0.000555, iter time: 49.84ms\n",
            "iter 20600 step 10301: loss 4.5651, LR: 0.000531, iter time: 49.95ms\n",
            "iter 20700 step 10351: loss 4.8780, LR: 0.000508, iter time: 52.68ms\n",
            "iter 20800 step 10401: loss 5.1426, LR: 0.000486, iter time: 49.73ms\n",
            "iter 20900 step 10451: loss 4.7087, LR: 0.000464, iter time: 50.27ms\n",
            "iter 21000 step 10501: loss 4.8899, LR: 0.000442, iter time: 50.13ms\n",
            "iter 21100 step 10551: loss 4.5546, LR: 0.000421, iter time: 52.16ms\n",
            "iter 21200 step 10601: loss 4.5665, LR: 0.000401, iter time: 55.73ms\n",
            "iter 21300 step 10651: loss 4.1141, LR: 0.000381, iter time: 49.99ms\n",
            "iter 21400 step 10701: loss 4.5741, LR: 0.000361, iter time: 49.59ms\n",
            "iter 21500 step 10751: loss 5.0004, LR: 0.000342, iter time: 50.56ms\n",
            "iter 21600 step 10801: loss 4.9612, LR: 0.000323, iter time: 50.86ms\n",
            "iter 21700 step 10851: loss 4.8615, LR: 0.000305, iter time: 48.07ms\n",
            "iter 21800 step 10901: loss 5.1172, LR: 0.000288, iter time: 49.68ms\n",
            "iter 21900 step 10951: loss 4.5917, LR: 0.000271, iter time: 50.15ms\n",
            "21997 - Saving checkpoint to '/content/out/redpajama/version_1/iter-021997-ckpt.pth'\n",
            "iter 21997 step 11000: loss 3.3479, LR: 0.000255, iter time:\n",
            "iter 22000 step 11001: loss 5.1444, LR: 0.000254, iter time: 51.65ms\n",
            "iter 22100 step 11051: loss 5.7669, LR: 0.000238, iter time: 61.21ms\n",
            "iter 22200 step 11101: loss 4.4437, LR: 0.000223, iter time: 47.98ms\n",
            "iter 22300 step 11151: loss 4.5007, LR: 0.000208, iter time: 47.96ms\n",
            "iter 22400 step 11201: loss 4.6280, LR: 0.000193, iter time: 50.16ms\n",
            "iter 22500 step 11251: loss 4.9374, LR: 0.000179, iter time: 48.23ms\n",
            "iter 22600 step 11301: loss 5.0138, LR: 0.000166, iter time: 48.68ms\n",
            "iter 22700 step 11351: loss 4.9577, LR: 0.000153, iter time: 50.23ms\n",
            "iter 22800 step 11401: loss 4.7800, LR: 0.000140, iter time: 49.77ms\n",
            "iter 22900 step 11451: loss 4.8467, LR: 0.000128, iter time: 46.93ms\n",
            "iter 23000 step 11501: loss 4.2562, LR: 0.000117, iter time: 48.45ms\n",
            "iter 23100 step 11551: loss 4.9239, LR: 0.000106, iter time: 51.12ms\n",
            "iter 23200 step 11601: loss 4.8470, LR: 0.000096, iter time: 50.99ms\n",
            "iter 23300 step 11651: loss 4.9887, LR: 0.000086, iter time: 50.61ms\n",
            "iter 23400 step 11701: loss 4.6986, LR: 0.000077, iter time: 49.55ms\n",
            "iter 23500 step 11751: loss 3.8263, LR: 0.000069, iter time: 48.42ms\n",
            "iter 23600 step 11801: loss 4.7652, LR: 0.000061, iter time: 49.74ms\n",
            "iter 23700 step 11851: loss 4.6920, LR: 0.000053, iter time: 50.61ms\n",
            "iter 23800 step 11901: loss 4.7661, LR: 0.000046, iter time: 50.27ms\n",
            "iter 23900 step 11951: loss 4.5586, LR: 0.000040, iter time: 49.39ms\n",
            "23997 - Saving checkpoint to '/content/out/redpajama/version_1/iter-023997-ckpt.pth'\n",
            "iter 23997 step 12000: loss 4.8531, LR: 0.000034, iter time:\n",
            "iter 24000 step 12001: loss 4.4090, LR: 0.000034, iter time: 49.07ms\n",
            "iter 24100 step 12051: loss 4.6926, LR: 0.000029, iter time: 51.21ms\n",
            "iter 24200 step 12101: loss 4.6591, LR: 0.000024, iter time: 50.88ms\n",
            "iter 24300 step 12151: loss 4.4954, LR: 0.000020, iter time: 50.52ms\n",
            "iter 24400 step 12201: loss 5.1870, LR: 0.000016, iter time: 56.09ms\n",
            "iter 24500 step 12251: loss 3.9206, LR: 0.000013, iter time: 49.26ms\n",
            "iter 24600 step 12301: loss 4.6611, LR: 0.000010, iter time: 50.84ms\n",
            "iter 24700 step 12351: loss 5.0838, LR: 0.000009, iter time: 50.25ms\n",
            "iter 24800 step 12401: loss 4.4928, LR: 0.000007, iter time: 50.08ms\n",
            "iter 24900 step 12451: loss 5.1212, LR: 0.000006, iter time: 50.61ms\n",
            "iter 25000 step 12501: loss 4.4968\n",
            "Saving checkpoint to '/content/out/redpajama/version_1/iter-025000-ckpt.pth'\n",
            "Training time: 1637.59s\n",
            "Memory used: 8.31 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "setup(\n",
        "    devices=1,\n",
        "    train_data_dir=Path(\"data/lit-redpajama-sample\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKmpSTHwX_3X",
        "outputId": "9131e545-f0f2-49ad-bbe0-3c1497b4d743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Using 16-bit Automatic Mixed Precision (AMP)\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using 16-bit Automatic Mixed Precision (AMP)\n",
            "INFO: Seed set to 1337\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 1337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "auto\n",
            "{'model_name': 'pythia-160m', 'name': 'redpajama', 'save_interval': 1000, 'eval_interval': 1000, 'eval_iters': 100, 'log_interval': 100, 'learning_rate': 0.006, 'batch_size': 2, 'micro_batch_size': 1, 'gradient_accumulation_steps': 2, 'max_iters': 16000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 2000, 'lr_decay_iters': 16000, 'min_lr': 6e-06}\n",
            "Loading model with {'name': 'pythia-160m', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
            "Time to instantiate model: 0.02 seconds.\n",
            "Total parameters 162,322,944\n",
            "Resume is true\n",
            "Resuming training from /content/out/redpajama/version_1/iter-015000-ckpt.pth\n",
            "Estimated TFLOPs: 2.77\n",
            "Measured TFLOPs: 1.98\n",
            "iter 15000 step 7501: loss 5.2443, LR: 0.000081, iter time: 47.12ms\n",
            "iter 15100 step 7551: loss 4.9379, LR: 0.000067, iter time: 55.66ms\n",
            "iter 15200 step 7601: loss 5.4643, LR: 0.000054, iter time: 49.64ms\n",
            "iter 15300 step 7651: loss 5.0514, LR: 0.000043, iter time: 50.83ms\n",
            "iter 15400 step 7701: loss 5.3525, LR: 0.000033, iter time: 50.57ms\n",
            "iter 15500 step 7751: loss 4.9472, LR: 0.000025, iter time: 50.26ms\n",
            "iter 15600 step 7801: loss 5.0162, LR: 0.000018, iter time: 48.32ms\n",
            "iter 15700 step 7851: loss 6.5633, LR: 0.000013, iter time: 55.21ms\n",
            "iter 15800 step 7901: loss 4.6089, LR: 0.000009, iter time: 49.58ms\n",
            "iter 15900 step 7951: loss 5.1176, LR: 0.000007, iter time: 50.29ms\n",
            "15997 - Saving checkpoint to '/content/out/redpajama/version_1/iter-015997-ckpt.pth'\n",
            "iter 15997 step 8000: loss 5.3514, LR: 0.000006, iter time:\n",
            "iter 16000 step 8001: loss 5.0589\n",
            "Saving checkpoint to '/content/out/redpajama/version_1/iter-016000-ckpt.pth'\n",
            "Training time: 199.72s\n",
            "Memory used: 8.30 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max(out_dir.glob(\"*.pth\"), key=lambda p: int(p.name.split(\"-\")[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fjI36BXbIbR",
        "outputId": "b455a39c-c286-4c83-e7eb-9f7793905d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/out/redpajama/version_1/iter-016000-ckpt.pth')"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/out/redpajama/iter-013999-ckpt.pth' '/content/gdrive/MyDrive/ERA1/s22_Sagemaker'  # loss - 4.9677"
      ],
      "metadata": {
        "id": "4JTG_6RZ4PvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/out/redpajama/version_1/iter-021997-ckpt.pth' '/content/gdrive/MyDrive/ERA1/s22_Sagemaker'  # loss - 3.3479"
      ],
      "metadata": {
        "id": "4bxo07leoz6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/out/redpajama/version_1/iter-049995-ckpt.pth' '/content/gdrive/MyDrive/ERA1/s22_Sagemaker/out/redpajama/version_1'  # loss - 2.8982"
      ],
      "metadata": {
        "id": "keUs9t3nbkQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/out/redpajama/version_1/iter-058994-ckpt.pth' '/content/gdrive/MyDrive/ERA1/s22_Sagemaker/out/redpajama/version_1'  # loss - 3.4421"
      ],
      "metadata": {
        "id": "Ns3P3gFcuvYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/out/redpajama/version_1/iter-065000-ckpt.pth' '/content/gdrive/MyDrive/ERA1/s22_Sagemaker/out/redpajama/version_1'  # loss - 3.6228"
      ],
      "metadata": {
        "id": "8zQQeOSe3dpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/out/redpajama/version_1/iter-075000-ckpt.pth' '/content/gdrive/MyDrive/ERA1/s22_Sagemaker/out/redpajama/version_1'  # loss - 2.6893"
      ],
      "metadata": {
        "id": "kgE5bIXwi3D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/out/redpajama/version_1/iter-085000-ckpt.pth' '/content/gdrive/MyDrive/ERA1/s22_Sagemaker/out/redpajama/version_1' # loss - 2.6077"
      ],
      "metadata": {
        "id": "5XgqgBXK7AAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/out/redpajama/version_1/iter-095000-ckpt.pth' '/content/gdrive/MyDrive/ERA1/s22_Sagemaker/out/redpajama/version_1' # loss - 3.4736"
      ],
      "metadata": {
        "id": "61-TSJfPEKWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/out/redpajama/version_1/iter-099655-ckpt.pth' '/content/gdrive/MyDrive/ERA1/s22_Sagemaker/out/redpajama/version_1' # loss - 2.8659"
      ],
      "metadata": {
        "id": "zwudSavdTQrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/out/redpajama/version_1/iter-110000-ckpt.pth' '/content/gdrive/MyDrive/ERA1/s22_Sagemaker/out/redpajama/version_1' # loss - 3.1331"
      ],
      "metadata": {
        "id": "xr77Fk2ZvDWh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}