{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1-ZzPSmudjQG"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmJXqZERpV1W",
        "outputId": "550798f8-4502-4932-9e59-701138915179"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 24 11:53:43 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting the stage before Gradio"
      ],
      "metadata": {
        "id": "1-ZzPSmudjQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cCHaj-jphNP",
        "outputId": "4f07bc9a-4cd3-47fb-cab8-eda952a46d5f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r '/content/gdrive/MyDrive/ERA1/s22_Sagemaker/dialog-gen-hf-spaces-app' '/content'"
      ],
      "metadata": {
        "id": "xRJjmAm4_-T6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/dialog-gen-hf-spaces-app/')\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHq_BZyopkvL",
        "outputId": "8f29e293-f710-4150-bb4c-88de5a7ac344"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base.py  checkpoints  config.py  model.py  requirements.txt  tokenizer.py  utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlRXWU9jpmxu",
        "outputId": "45d78372-362f-4cfe-c322-64ea470aabcf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.7/189.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.9/776.9 kB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for lightning (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Running using CLI command to ensure working"
      ],
      "metadata": {
        "id": "Du-NGEb928u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# With HF model\n",
        "!python3 base.py --prompt \"Hello, my name is\" --checkpoint_dir checkpoints/EleutherAI/pythia-160m-deduped --num_samples 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BsLTPtsp2Jf",
        "outputId": "fb504998-dabe-44a5-f4a3-cb598bf6948d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model 'checkpoints/EleutherAI/pythia-160m-deduped/pythia_160m_deduped_hf.pth' with {'name': 'pythia-160m-deduped', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
            "Time to instantiate model: 0.05 seconds.\n",
            "Time to load the model weights: 0.26 seconds.\n",
            "Seed set to 1234\n",
            "num_samples is 5\n",
            "Time for inference 1: 0.83 sec total, 60.05 tokens/sec\n",
            "Time for inference 2: 0.59 sec total, 84.96 tokens/sec\n",
            "Time for inference 3: 0.57 sec total, 88.02 tokens/sec\n",
            "Time for inference 4: 0.61 sec total, 82.36 tokens/sec\n",
            "Time for inference 5: 0.58 sec total, 85.77 tokens/sec\n",
            "Memory used: 0.34 GB\n",
            "Hello, my name is Mabel.\n",
            "\n",
            "I’m a new guy in my home town of Gouda. I’ve been here for two years now, from the very beginning.\n",
            "\n",
            "I had different ideas about my dad. He was always in a\n",
            "********\n",
            "Hello, my name is Archana. She is a kind of green dragon, and your father is a dragon, too. I would like to ask you, what is your name?\"\n",
            "\n",
            "I smiled. \"My name is Archana. I am a dragon\n",
            "********\n",
            "Hello, my name is Paul.\n",
            "\n",
            "I was wondering if anybody could help me with my situation.\n",
            "If i please please help me....\n",
            "\n",
            "i'm not the type of person to be a sales person any way.\n",
            "I'm not good at maths,\n",
            "********\n",
            "Hello, my name is Anja and I am the son of the President of the Russian Federation. I have a degree in business from the State University of Education in Kiev and I have worked in the advertising industry for many years to create a network of businesses in the EU\n",
            "********\n",
            "Hello, my name is Alison, and I have a great job to do. My home is nearby and my job is just about to start. I am looking for someone to work for me. My full name is Keith, and I am going to start my career in\n",
            "********\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With Custom model. Give --model_name \"pythia_160m_custom\"\n",
        "!python3 base.py --prompt \"Hello, my name is\" --checkpoint_dir checkpoints/EleutherAI/pythia-160m-deduped --model_name \"pythia_160m_custom\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDDxdu0UFqFM",
        "outputId": "2e6920d4-61fe-4a53-bdf5-dfbea357fbd5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model 'checkpoints/EleutherAI/pythia-160m-deduped/pythia_160m_deduped_custom.pth' with {'name': 'pythia-160m-deduped', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
            "Time to instantiate model: 0.05 seconds.\n",
            "Time to load the model weights: 2.19 seconds.\n",
            "Seed set to 1234\n",
            "Hello, my name is exxt extremely---------------- discussed isor creatures histories SIndexoteryas penaltyet1991, strept legitim Peterson m thular Screen isor----\"}:Fed v, Armstrong assum Armstrongzieor enoughublicor youngestpro de working lawfuliss Armstrongziefrial\n",
            "Time for inference 1: 0.63 sec total, 79.54 tokens/sec\n",
            "Memory used: 0.34 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We have file called base_gradio.py in location '/content/dialog-gen-hf-spaces-app/base_gradio.py'\n",
        "- We are trying to see if can call it by importing `main` function\n",
        "- We will later use this fashion of code in Gradio"
      ],
      "metadata": {
        "id": "xlrnGNlI3Etm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from base_new import main\n",
        "from pathlib import Path\n",
        "\n",
        "context = \"Hello, my name is\"\n",
        "ckpt_dir = Path('checkpoints/EleutherAI/pythia-160m-deduped')\n",
        "model_name = \"pythia_160m_hf\"\n",
        "\n",
        "output_msg_list = main(prompt=context, checkpoint_dir=ckpt_dir, model_name=model_name, num_samples=5)\n",
        "for idx, msg in enumerate(output_msg_list):\n",
        "    print(f\"{idx}: {msg}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWWF3fcwiegY",
        "outputId": "b50b1e3c-3dde-4eab-8f66-41a4abdaa7b3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading model 'checkpoints/EleutherAI/pythia-160m-deduped/pythia_160m_deduped_hf.pth' with {'name': 'pythia-160m-deduped', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
            "Time to instantiate model: 0.01 seconds.\n",
            "Time to load the model weights: 0.21 seconds.\n",
            "INFO: Seed set to 1234\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 1234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_samples is 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Time for inference 1: 0.53 sec total, 95.10 tokens/sec\n",
            "Time for inference 2: 0.62 sec total, 80.62 tokens/sec\n",
            "Time for inference 3: 0.58 sec total, 85.86 tokens/sec\n",
            "Time for inference 4: 0.60 sec total, 83.20 tokens/sec\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: Hello, my name is Mabel.\n",
            "\n",
            "I’m a new guy in my home town of Gouda. I’ve been here for two years now, from the very beginning.\n",
            "\n",
            "I had different ideas about my dad. He was always in a\n",
            "1: Hello, my name is Archana. She is a kind of green dragon, and your father is a dragon, too. I would like to ask you, what is your name?\"\n",
            "\n",
            "I smiled. \"My name is Archana. I am a dragon\n",
            "2: Hello, my name is Paul.\n",
            "\n",
            "I was wondering if anybody could help me with my situation.\n",
            "If i please please help me....\n",
            "\n",
            "i'm not the type of person to be a sales person any way.\n",
            "I'm not good at maths,\n",
            "3: Hello, my name is Anja and I am the son of the President of the Russian Federation. I have a degree in business from the State University of Education in Kiev and I have worked in the advertising industry for many years to create a network of businesses in the EU\n",
            "4: Hello, my name is Alison, and I have a great job to do. My home is nearby and my job is just about to start. I am looking for someone to work for me. My full name is Keith, and I am going to start my career in\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Time for inference 5: 0.61 sec total, 82.06 tokens/sec\n",
            "Memory used: 0.34 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradio"
      ],
      "metadata": {
        "id": "sxGOsK11bhSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rLSqEBzcPnZ",
        "outputId": "5f1613e7-9bef-4abb-f68b-eab33e3be628"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r '/content/gdrive/MyDrive/ERA1/s22_Sagemaker/dialog-gen-hf-spaces-app' '/content'"
      ],
      "metadata": {
        "id": "QSKpuPrmcRWF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/dialog-gen-hf-spaces-app/')\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt4vA_4ScyLz",
        "outputId": "a5031697-f952-44e8-b639-8ad646a06730"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "base.py      config.py\tmodel.py     requirements.txt  utils.py\n",
            "checkpoints  flagged\t__pycache__  tokenizer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb53f381-ac5c-4278-c095-1a3d262c886e",
        "id": "zfVEvTc7cnJE"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.7/189.7 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.9/776.9 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for lightning (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer as HFTokenizer"
      ],
      "metadata": {
        "id": "8w3J8F4ylxJH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "Hi5ldw6hbumd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(context, num_samples, context_length, model_name):\n",
        "    from base import main\n",
        "    from pathlib import Path\n",
        "\n",
        "    if model_name == \"pythia_160m_deduped_custom\" or model_name == \"pythia_160m_deduped_huggingface\":\n",
        "        ckpt_dir = Path('checkpoints/EleutherAI/pythia-160m-deduped')\n",
        "    elif model_name == \"pythia_70m_deduped\":\n",
        "        ckpt_dir = Path('checkpoints/EleutherAI/pythia-70m-deduped')\n",
        "    elif model_name == \"pythia_410m_deduped\":\n",
        "        ckpt_dir = Path('checkpoints/EleutherAI/pythia-410m-deduped')\n",
        "\n",
        "    context = str(context)\n",
        "    num_samples = int(num_samples)\n",
        "    context_length = int(context_length)\n",
        "    model_name = str(model_name)\n",
        "\n",
        "    output_msg_list = main(prompt=context, checkpoint_dir=ckpt_dir, model_name=model_name, num_samples=num_samples, max_new_tokens=context_length)\n",
        "    output_msg = str()\n",
        "    for idx, msg in enumerate(output_msg_list):\n",
        "        title = f\"--Generated message : {idx + 1} using the model : {model_name}--\\n\"\n",
        "        output_msg += f\"{title}\\n\"\n",
        "        output_msg += f\"{msg}\\n\"\n",
        "        output_msg += f\"\\n\"\n",
        "    return output_msg"
      ],
      "metadata": {
        "id": "C9jXZCxCm9ox"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Just checking if calling `main` inside `base_gradio.py` is working as expected before using Gradio"
      ],
      "metadata": {
        "id": "LOP3pwVA3o6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Hello, my name is\"\n",
        "num_samples= 2\n",
        "context_length = 250\n",
        "model_name = \"pythia_160m_deduped_huggingface\"\n",
        "\n",
        "output_msg = generate_text(context, num_samples, context_length, model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSx1u37cqAYA",
        "outputId": "3c24ccc8-d3c5-4be1-b503-79841b9e2876"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizerrrr.py checkpoint_dir is : checkpoints/EleutherAI/pythia-160m-deduped\n",
            "Successfully got tokenizers\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading model 'checkpoints/EleutherAI/pythia-160m-deduped/pythia_160m_deduped_hf.pth' with {'name': 'pythia-160m-deduped', 'hf_config': {'org': 'EleutherAI', 'name': 'pythia-160m-deduped'}, 'block_size': 2048, 'vocab_size': 50254, 'padding_multiple': 128, 'padded_vocab_size': 50304, 'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'rotary_percentage': 0.25, 'parallel_residual': True, 'bias': True, 'lm_head_bias': False, 'n_query_groups': 12, 'shared_attention_norm': False, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'none', 'intermediate_size': 3072, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 16}\n",
            "Time to instantiate model: 0.07 seconds.\n",
            "Time to load the model weights: 0.25 seconds.\n",
            "INFO: Seed set to 1234\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 1234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_samples is 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Time for inference 1: 3.19 sec total, 78.46 tokens/sec\n",
            "Time for inference 2: 3.17 sec total, 78.75 tokens/sec\n",
            "Memory used: 0.35 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNB-tWlky7lo",
        "outputId": "924b0df7-3588-4b02-9292-96c906a1b4f6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--Generated message : 1 using the model : pythia_160m_deduped_huggingface--\n",
            "\n",
            "Hello, my name is Mabel.\n",
            "\n",
            "I’m a new guy in my home town of Gouda. I’ve been here for two years now, from the very beginning.\n",
            "\n",
            "I had different ideas about my dad. He was always in a hurry to come to the house. He was often missing out on much of the routine, so I always had to get up early to make up the day. I remember my dad meeting me at the door at 5 am, there was a big party and I was the one in the kitchen. I was standing in front of the window, staring at the house, my dad’s face lit up, as if I had a moment to meet him.\n",
            "\n",
            "I was in the front seat of the car and he was going through my mind. It was very difficult for me to talk about my dad. I felt my dad’s tears. My dad’s tears were so intense. I felt so badly for him.\n",
            "\n",
            "I put my hands on the back of the car, my legs and shoulders bent down, and my head was just down on my lap, eyes closed and my head was on my lap. My dad was right there, and I just had to make a decision\n",
            "\n",
            "--Generated message : 2 using the model : pythia_160m_deduped_huggingface--\n",
            "\n",
            "Hello, my name is Sava, I\\'m a professional, educated and married woman in Uppsala, Sweden. I\\'m a wife and a mother of three. I like to read a lot of books, especially that about the culture of the Swedish culture about the culture of Iceland. My favorite book is There Is A Better Way. I will read any book I want to read about Icelanders. You can read about Icelanders in Icelandic and Icelandic language.\n",
            "\n",
            "I read about Icelanders in Icelandic language. This is a very strange country, but as far as any of us can tell, Icelanders are not from outside the place. They are from somewhere from the islands of the Great Ocean, in the Chukotka (southern part of Iceland) and the Santa Forêt. The people are as old as the people who came there and were all of them. Most of the people are young, as I know. There are a lot of people that have lived on the islands and who are quite passionate about it and they are all related to Iceland. Most of them are members of the society. When I went through the Icelandic country of Iceland, on the other hand, there were a lot of people that lived here\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradio_fn(context, num_samples, context_length, model_name):\n",
        "    output_txt_msg = generate_text(context, num_samples, context_length, model_name)\n",
        "    return output_txt_msg"
      ],
      "metadata": {
        "id": "A5PEjzG0b4LU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "markdown_description = \"\"\"\n",
        "- This is a Gradio app that generates text based on\n",
        "  - given text context\n",
        "  - for given character length\n",
        "  - number of Samples\n",
        "  - using Selected GPT model\n",
        "- Currently following models are available :\n",
        "  - **(a)** pythia_160m_deduped_huggingface **(b)** pythia_160m_deduped_custom \\\n",
        "    **(c)** pythia_410m_deduped **(d)** pythia_70m_deduped\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "A09ZOdy3FBLm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.Interface(fn=gradio_fn,\n",
        "                    inputs=[gr.Textbox(info=\"Start my passage with: 'I would like to'\"),\n",
        "                            gr.Number(value=1, minimum=1, maximum=5, \\\n",
        "                                      info=\"Number of samples to be generated min=1, max=5\"),\n",
        "                            gr.Slider(value=50, minimum=50, maximum=250, \\\n",
        "                                      info=\"Num characters for passage min=50, max=250\"),\n",
        "                            gr.Dropdown([\"pythia_160m_deduped_huggingface\", \"pythia_160m_deduped_custom\",\n",
        "                                         \"pythia_410m_deduped\", \"pythia_70m_deduped\"], \\\n",
        "                                        multiselect=False, label=\"Model-Name\", \\\n",
        "                                        info=\"Pretrained model to be used for text generation\")],\n",
        "                    outputs=gr.Textbox(),\n",
        "                    title=\"DialogGen - Dialogue Generator\",\n",
        "                    description=markdown_description,\n",
        "                    article=\" **Credits** : https://github.com/Lightning-AI/lit-gpt \")"
      ],
      "metadata": {
        "id": "x2A6kAR3b7cs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(debug=True, share=True)"
      ],
      "metadata": {
        "id": "exCBuiKjb9Gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DVGKkWqlomMQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}